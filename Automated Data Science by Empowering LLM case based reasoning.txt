

----- Page 1 -----

DS-Agent: Automated Data Scienceby Empowering Large Language Models with Case-Based ReasoningSiyuan Guo123Cheng Deng4Ying Wen4Hechang Chen12Yi Chang123Jun Wang5AbstractIn this work, we investigate the potential of largelanguage models (LLMs) based agents to auto-mate data science tasks, with the goal of com-prehending task requirements, then building andtraining the best-ﬁt machine learning models. De-spite their widespread success, existing LLMagents are hindered by generating unreasonableexperiment plans within this scenario. To this end,we present DS-Agent, a novel automatic frame-work that harnesses LLM agent and case-basedreasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure anautomatic iteration pipeline, which can ﬂexiblycapitalize on the expert knowledge from Kaggle,and facilitate consistent performance improve-ment through the feedback mechanism. Moreover,DS-Agent implements a low-resource deploymentstage with a simpliﬁed CBR paradigm to adaptpast successful solutions from the developmentstage for direct code generation, signiﬁcantly re-ducing the demand on foundational capabilitiesof LLMs. Empirically, DS-Agent with GPT-4achieves 100% success rate in the developmentstage, while attaining 36% improvement on av-erage one pass rate across alternative LLMs inthe deployment stage. In both stages, DS-Agentachieves the best rank in performance, costing$1.60 and$0.13 per run with GPT-4, respectively.Our data and code are open-sourced athttps://github.com/guosyjlu/DS-Agent.1School of Artiﬁcial Intelligence, Jilin University2EngineeringResearch Center of Knowledge-Driven Human-Machine In-telligence, Jilin University3International Center of FutureScience, Jilin University4Shanghai Jiao Tong University5University College London. Correspondence to: Hechang Chen<chenhc@jlu.edu.cn>, Yi Chang<yichang@jlu.edu.cn>, JunWang<jun.wang@cs.ucl.ac.uk>.Proceedings of the41stInternational Conference on MachineLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 bythe author(s).1. IntroductionRecently, the remarkable foundational capabilities of largelanguage models (LLMs) (OpenAI,2022;2023) have em-powered autonomous language agents to address a widespectrum of tasks effectively (Brohan et al.,2023;Kim et al.,2023;Shen et al.,2023;Boiko et al.,2023;Romera-Paredeset al.,2023). In this work, we explore an open-ended deci-sion making scenario-automated data science (De Bie et al.,2022;Mahadi Hassan et al.,2023), which aims at democra-tizing access to data insights while minimizing the need forspecialized expertise. Speciﬁcally, our focus is on automat-ing machine learning (ML), a particularly specialized part,with the primary goal of comprehending task requirements,building and training the best-ﬁt ML models, and ﬁnallydeploying the trained model.Despite the widespread success of LLM agents, a recentwork (Huang et al.,2023) indicates that existing agents, in-cluding AutoGPT (Signiﬁcant Gravitas,2023), LangChain(Chase,2022), and the state-of-the-art ResearchAgent(Huang et al.,2023), struggle to achieve a high task com-pletion rate within the data science scenario, even whenimplemented with the most powerful LLM GPT-4. This ismainly attributed to LLMs’ deﬁciency in generating reason-able plans and their hallucination issues. To mitigate this,a promising solution is to further ﬁnetune LLMs to alignwith the automated data science scenario (Carta et al.,2023;Zeng et al.,2023;Chen et al.,2023;Christianos et al.,2023).Nevertheless, collecting sufﬁcient samples for ﬁnetuningposes a signiﬁcant challenge due to the time costs involved,particularly because feedback from automated data sciencetasks necessitates the completion of code execution. Worsestill, since LLMs typically have billions of parameters, theback-propagation and optimization during the ﬁnetuningleads to intensive computation resources.In this context, Kaggle emerges as a pivotal resource. As theworld’s largest data science competition platform, it boasts avast repository of technical reports and codes contributed bya community of seasoned data scientists. To empower LLMagents to harness this wealth of expert knowledge efﬁciently,we turn to a classical AI problem-solving paradigm–case-based reasoning (CBR) (Kolodner,1992;Watson & Marir,1994). The CBR framework operates by retrieving similar1arXiv:2402.17453v5  [cs.LG]  28 May 2024


----- Page 2 -----

DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoningpast problems, reusing their solutions for the current prob-lem, evaluating the effectiveness, revising the solution, andretaining successful solutions. Utilizing CBR enables LLMagents to analyze, extract and reuse solution patterns fromthese human insights, and to iteratively revise the solutionbased on the execution feedback to attain consistently im-proved performance. The integration of CBR into LLMagents not only enhances their problem-solving abilities indata science tasks but also achieves high efﬁciency in bothsample and computation resources.To this end, we propose DS-Agent, a novel framework thatharnesses LLM agent and CBR to facilitate model-centricautomated data science, as depicted inFigure 1(a). Overall,DS-Agent operates in two distinct stages: the standard de-velopment stage, and low-resource deployment stage. Forthe development stage, DS-Agent builds on top of CBRframework to capitalize on the collected human insightsfrom Kaggle, structuring an automatic iteration pipeline.Given a new task, DS-Agent retrieves and reuses relevanthuman insights from Kaggle to develop an experiment plan,and then iteratively adjusts the retrieved case and revisesthe experiment plan in response to the execution feedback.Beneﬁting from the CBR framework, DS-Agent can lever-age expert knowledge from Kaggle to develop groundedexperiment plans, while providing a ﬂexible learning mech-anism by retaining successful solutions to the case bankinstead of resource-intensive parameter updates throughback-propagation. Moreover, the feedback mechanism inCBR allows DS-Agent to iteratively retrieve useful casesand revise the experiment plan, achieving consistent perfor-mance improvement, as shown inFigure 1(b).In the deployment stage, DS-Agent employs a simpliﬁedCBR framework for a low-resource scenario, where the taskis code generation directly in response to the user’s taskrequirements without iterative revise based on executionfeedback. Particularly, DS-Agent retrieves and reuses pastsuccessful solutions collected from the development stagefor the current task. As such, DS-Agent beneﬁts from thesimpliﬁed CBR framework to facilitate the knowledge trans-fer from past solutions to solve an unseen deployment taskin the same task distribution. With a similar solution casein the context, DS-Agent necessitates only making minormodiﬁcations for adaptation, thereby signiﬁcantly reducingthe demands on foundational capabilities of LLMs.Empirically, we demonstrate the superiority of DS-Agentacross30data science tasks in both stages. During thedevelopment stage, DS-Agent with GPT-4 achieves 100%success rate across12tasks. For the deployment stage, DS-Agent with GPT-3.5 and GPT-4 achieves 85% and 99%one pass rate over18deployment tasks, while the bestbaseline attains a mere 56% and 60%. Remarkably, DS-Agent improves the one pass rate of an open-source LLMNew Task
RetrieveRankReviseReuseExecuteRetainAdaptCase Bank(a) Development Stage(b) Deployment Stage
Revise Loop(a)(b)Figure 1.(a) Overview of DS-Agent with CBR based LLMs. (b)Performance improvement of DS-Agent with increasing iterationsteps by CBR over 12 development tasks.Mixtral-8x7b-Instructfrom a mere 6% to 31%. Inboth stages, DS-Agent with GPT-4 and GPT-3.5 attains thehighest and second-highest ranks in performance. More-over, DS-Agent costs$0.06 and$1.60 per run for GPT-3.5and GPT-4 with standard scenario, which reduce further to$0.0045 and$0.135 in low-resource scenarios, renderingDS-Agent highly appealing for real-world deployment.2. PreliminaryCBR based LLMs.CBR (Kolodner,1992;Watson & Marir,1994) is a classical AI paradigm, which solves a new taskby retrieving other similar problems, reusing their solu-tions, evaluating the effectiveness, and iteratively revisingthe solution as needed. The solution with best evaluationperformance is retained to the database for future reuse. Inthis work, we integrate the CBR framework into LLMs toenhance their problem-solving capabilities. As shown inFigure 2(b), CBR based LLMs encompass three compo-nents: (i) a retrieverpRthat returns the distributions over thedatabase based on the taskωand feedbackl, (ii) an LLMpLLMthat generates the solutionywith the taskω, feedbackland the retrieved casec, and (iii) an evaluatorpEthat pro-duces feedbacklof the solutiony. Formally, CBR basedLLMs encompass an iteration loop with thet-th step aspCBR(yt|ω)=/summationdisplaylt→1pE(lt→1|ω)/summationdisplayctpR(ct|ω,lt→1)pLLM(yt|ct,ω ,lt→1),(1)where the solution distribution is marginalized by executionfeedback of the last steplt→1and the retrieved casect. Then,the feedback distribution can be formulated aspE(lt|ω)=/summationdisplayytpCBR(yt|ω)pE(lt|yt,ω).(2)As such, the feedback is produced by evaluating the currentsolution distribution, and the subsequent solution distribu-tion is revised based on the current feedback, thus formingan iteration loop with consistent performance improvement.2


----- Page 3 -----

DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning
Database(a) RAG based LLMs(b) CBR based LLMs
Task !Solution "!Retriever !!LLM !""#Case #Database
Task !Solution "!!Retriever !!LLM !""#Case #!$!"#$!"#Evaluation !$Feedback $!"with best evaluation performance RetainFigure 2.Comparison between (a) RAG based LLMs and (b) CBRbased LLMs.Comparison with Retrieval-Augmented Generation.CBR based LLMs exhibit similarity with retrieval-augmented generation (RAG) (Rubin et al.,2022;Wanget al.,2023b;Gao et al.,2023) - both involve the retrievaland reuse. As shown inFigure 2(a), RAG based LLMs onlyinvolve a retriever and an LLM, and can be formulated aspRAG(y|ω)=/summationdisplaycpR(c|ω)pLLM(y|c, ω),(3)where the solution distribution is marginalized by a singlelatent variable, i.e., the retrieved casec. Therefore, whileboth LLMs can retrieve and reuse solution patterns fromthe retrieved case, CBR based LLMs can additionally adjustthe retrieved the case and revise the solution in responseto the evaluation feedback. Moreover, retaining good solu-tions to the database enables CBR based LLMs to achieve aﬂexible learning mechanism, thereby leading to consistentperformance improvement.3. The DS-AgentIn this section, we present DS-Agent, an automatic frame-work that leverages LLM agent and CBR to solve datascience tasks. As shown inFigure 3, DS-Agent oper-ates in two stages: the development stage and the deploy-ment stage. Each stage handles the corresponding tasksetTdevelopandTdeploy, where the task is deﬁned as a tuple(ω,Dtrain,Dvalid,Dtest,M). For both stages, DS-Agent com-prehends the task descriptionω, generates code to train anML model with the training setDtrain, and evaluates its per-formance on the validation setDvalidusing the evaluationmetricM. We report the performance of the trained MLmodels on the test setDtest.3.1. Development Stage: Automatic Iteration PipelineIn the development stage, we structure the workﬂow ofDS-Agent to emulate the iterative process a data scientistfollows of building, training, and validating ML modelsgiven a data science task. However, since LLMs are not in-herently trained for data science scenarios, they lack preciseknowledge to generate reasonable plans for the design ofML models, which leads to an unreliable task completionrate (Huang et al.,2023). Kaggle, as a leading platform fordata science competitions, offers a rich repository of expertinsights and solutions that employ cutting-edge ML tech-niques. By integrating these practical, expert examples intoLLM agents, we can signiﬁcantly improve their capabilityto solve complex data science tasks. To this end, we pro-pose to integrate CBR into the automatic iteration pipelineof DS-Agent as shown inFigure 3(a). Now, we elaborateon the automatic iteration pipeline of DS-Agent as follows.Human Insight Case Collection.Our primary objectiveis to collect expert insights and solutions with advancedML techniques from Kaggle. Concretely, we select severalrecently completed Kaggle competitions, concentrating onthree data modalities: text, time series and tabular data. Thisaligns with the modalities present in the development anddeployment task set in this work. From selected compe-titions, we crawl both the technical reports shared by thewinning teams and the codes with top-ranked scores in thepublic leaderboard. These materials undergo a reformula-tion process: technical reports are cleaned to reserve coreinsights, while the code is summarized using GPT-3.5 toconvert complex implementation into textual insights. Then,they are stored into the human insight case bank.Step 1: Retrieve.First, DS-Agent retrieves relevant casesfrom the human insight case bankCthat pertain to the cur-rent data science task. Particularly,Retrievercalculatesthe similarity between the task descriptionωand casec→Cwith their cosine similarity:sim(ω,c) = cos(E(ω),E(c)),whereE(·)denotes the pretrained embedding model. Then,the top-kcases with the highest similarities to the task de-scription are retrieved in this step.Step 2: ReviseRank.While the above step generally en-sures the relevance of the retrieved cases, it cannot dynami-cally adjust the retrieved cases in response to the executionfeedback of the previous iteration. One possible solution isto ﬁnetune the retriever using the execution feedback (Shiet al.,2023;Wang et al.,2023a). Yet, automated data sci-ence tasks present a unique challenge as they require codeexecution to produce feedback, incurring considerable timeand computational expenses. To address this issue, we pro-pose to harness the LLMs’ capability to estimate the utilityof the retrieved cases by analyzing the execution feedback,and then revise the ranking order to adjust the retrieved case.3


----- Page 4 -----

DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning
RankReviserRetriever
PlannerLoggerDebuggerProgrammer
Step 1: Retrieve
Case 1
Case 2
Case "…
Case 3
Case "
Case 2
…
Human InsightCase BankTaskDescription 
Experiment PlanWorkspaceResultPythonScriptErrorExecuteStep 2: ReviseRankStep 3: ReuseStep 4: Execute
Case 3
Revise Loop: Return to Step 2Step 5: Retain
AgentCase Bank
There are some texts. Your task is to identify whether it is generated by an AI. The evaluation metric is accuracy. Please provide a python script to train the language model.Retriever
Adapter
ExampleCaseUser:
BestPerformance(a) Development Stage
(b) Deployment Stage
Experiment Log
Figure 3.The diagram of DS-Agent.(a) Development Stage:DS-Agent structures an automatic iteration pipeline to build and revise themodel based on execution feedback.(b) Deployment Stage:DS-Agent adapts past successful solutions for code generation.Inspired by a recent work (Sun et al.,2023) that leveragesLLMs for relevance ranking in web search scenarios, weadopt a similar prompt format to assign each of top-kre-trieved cases{c1,c2,. . . ,ck}with a unique identiﬁer (e.g.,[1], [2], etc.). We then prompt LLMs to generate the permu-tation of theses cases in descending order of their estimatedutility for the current data science task, as informed by thefeedback from the last iteration. The ranking results aregenerated in the format of [2]>[1]>[3], etc. Formally, atthe iteration stept, the utility distribution of each case is es-timated aspRR(c|ω,lt→1)=pLLM(c|c1,c2,. . . ,ck,ω ,lt→1),wherepLLMdenotes the distribution of the LLM,lt→1de-notes the execution feedback in the iteration stept↑1, ini-tialized as an empty string, i.e.,l0=↓. Then, the top-rankedcasectis proceeded for the next step of reuse. Consequently,within the automatic iteration pipeline, the case for reuseis iteratively reﬁned byReviseRankerin response tothe execution feedback. This dynamic adjustment providesDS-Agent with iteratively updated foundational materialsto revise the solution for the experimental plan.Step 3: Reuse.In this phase, DS-Agent employsPlannerto reuse the retrieved case to develop the solution for theexperiment plan. In the iteration stept,Plannerexaminesthe task descriptionωand the previous execution feedbacklt→1to comprehend the current context. Then, it thought-fully analyzes the top-ranked casect, reuses the humaninsights included to adapt to the current task, and ﬁnallydevelops a new solution of the experiment planyt.Step 4: Execute.Subsequently, DS-Agent implements theexperiment plan with a Python script, and executes it toderive the empirical feedback. In particular,Programmergoes through the task description and experiment plan togenerate the corresponding Python code. Following codegeneration, the script is executed to review the output. Iferrors are reported,Debuggeris employed to identify andresolve bugs. Taking inspirations from Reﬂexion (Shinnet al.,2023),Debuggerﬁrst reﬂects on potential bugs ac-cording to the execution feedback, and then generates andre-execute the corrected code. This debugging process con-tinues until there are no further errors reported, or until themaximum number of predeﬁned debugging attempts hasbeen exhausted. Finally,Loggeroutputs a comprehensivesummary of the concluded experiment’s progress and resultsof natural language form. The preservation of the experi-ment log provides DS-Agent with the execution feedback,enabling it to further revise the experiment plan to designbetter ML models for the current task.Step 5: Retain.At the end of each iteration step, we utilizethe trained ML model to make predictions on the test set.If improved performance is observed, DS-Agent archivesthe task descriptionωalong with the corresponding Pythonscriptsinto both human insight case bankCand agent casebankBas an example solution case for future reuse.Revise Loop: Return to Step 2.After the Retain step, theworkﬂow returns to the ReviseRank step to create a Reviseloop. This loop empowers DS-Agent to further revise thesolution for the experiment plan according to the executionfeedback of the current steplt. The Revise loop terminateswhen the predetermined maximum number of iteration stepshas been reached.With the aforementioned steps during the development stage,DS-Agent utilizes the CBR framework to iteratively retrieveand reuse relevant, effective case to revise the solution of theexperiment plan, leading to improved problem-solving ca-4

----- Page 5 -----

DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoningpabilities for data science tasks. Here, the CBR frameworkcan be formulated aspdevCBR(yt|ω)=/summationdisplaylt→1pE(lt→1|ω)·/summationdisplayc↑top-k(sim(ω,·))pRR(c|ω,lt→1)pLLM(yt|ω,c,lt→1),(4)which aligns with the solution distribution of CBR basedLLMs inEquation 1and the only difference is that we utilizebothRetrieverandReviseRankerto retrieve casesin response to the task and the execution feedback.We summarize the pseudo-code of the automatic pipelinein Algorithm1. Overall, DS-Agent beneﬁts from the CBRparadigm in two aspects. Firstly, CBR integrates the hu-man insight case bank, which contains intensive expertknowledge of the data science, enabling DS-Agent to de-rive reasonable experiment plans. Moreover, CBR offers aﬂexible learning mechanism by retaining successful solu-tion cases into the human insight case bank, thus eliminat-ing the need for resource-intensive ﬁnetuning of LLMs viaback-propagation. For instance, when encountering noveltasks that involve previously unseen data modalities, suchas graph data (Pei et al.,2020;2024a;b), it can simply inte-grate the latest human insights into the case bankC. Thisenables DS-Agent to adeptly solve data science tasks re-lated to graph data by drawing on its enriched knowledgerepository.Secondly, the Revise loop within CBR allows DS-Agentto utilize the execution feedback from the last iteration toguide the case retrieval and to revise the experiment planvia case reuse. This iterative loop leads to consistent perfor-mance improvement by progressively revising the design ofML models towards an optimal ﬁt. We plot the performancecurve of DS-Agent with the increasing iteration steps in Fig-ure1(b). A consistent trend of performance improvementis observed empirically.3.2. Deployment Stage: Learning from Past CasesIn the deployment stage, we aim to reuse the past successfulsolution cases archived in the agent case bankBto achievea low-resource scenario, where DS-Agent directly generatesthe Python code in response to the user’s task requirementsfor training ML models. Without the iteration loop, wesimplify the CBR paradigm of the development stage toimplement DS-Agent by adapting solution code from similartasks to the current ones.As shown in Figure3, DS-Agent ﬁrst retrieves relevantcase and then reuses the case to adapt to the deploy-ment tasks. Speciﬁcally, given a deployment taskω,DS-Agent ﬁrst retrieves a case pair(ω0,s0)from theagent case bankBwith similar task description, i.e.,(ω0,s0) = arg max(ω0,s0)↑Bsim(ω,ω0). Then, DS-AgentutilizesAdapterto reuse the retrieved example case pairfor adaptation to the current task, generating the solutioncode for training ML models. This simpliﬁed CBR frame-work can be formulated aspdepCBR(s|ω)=pLLM(s|arg max(ω0,s0)↑Bsim(ω,ω0),ω).(5)In the deployment stage, DS-Agent leverages a simpliﬁedCBR paradigm to facilitate the transfer of knowledge fromthe past successful cases to solve an unseen data sciencetask in the same task distribution. By providing a similar so-lution case in the context, DS-Agent necessitates only minormodiﬁcations to tailor it to the new task. This signiﬁcantlyeases the demands on the reasoning and coding capabilitiesof LLMs. As a result, DS-Agent can be implemented on topof even open-source LLMs for the deployment stage.4. Experiments4.1. Experiment SettingTask Selection.We select 30 data science tasks with threedata modalities, including text, time series and tabular data,and two fundamental task types of regression and classiﬁ-cation. These diverse datasets were sourced from a varietyof platforms. We incorporate various evaluation metrics forthese tasks. Out of the 30 tasks, 12 have been earmarkedfor the development stage, while the remaining 18 are des-ignated for deployment. For each dataset, we write naturallanguage task description, and split them into training set,validation set and the test set. Besides, we prepare a Pythonscript that establishes a baseline of random guess, serving asan initial reference point. The detailed dataset description ispresented in Table5.Evaluation Metric.We mainly evaluate the agent’s abilityfrom three aspects:(1) Completion of building ML mod-els.In the development stage, we employ the success rate,i.e., whether the agent can build an ML model in a bug-freemanner within a ﬁxed number of steps. In the deploymentstage, the metric is the one-pass rate, indicating the agent’sability to build an ML model with only a single trial.(2)Performance of built ML models.For both stages, weutilize mean rank and best rank as the evaluation metric toevaluate the agents’ capabilities for automated data science.(3) Resource cost.Since we mainly utilize closed-sourceLLMs in this work, we take the consumed money to assessresource costs.Please refer to AppendixBfor more experiment details.4.2. Results for Development Stage4.2.1. MAINRESULTSBaselines.In the development stage, we conduct a com-parative analysis between DS-Agent and ResearchAgent5


----- Page 6 -----

DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based ReasoningTable 1.Mean rank and best rank w.r.t. task-speciﬁc evaluation metric results on 12 data science tasks in the development stage. Resultsare reported over ﬁve repetitive trials. Best performances are highlighted in bold, and second best performances are underlined.FB AR TE CP ETT ILI HW EC MCS WBY ST ESAvgMean RankGPT-3.5ResearchAgent8.0 10.0 12.0 13.0 9.4 11.0 14.2 12.2 15.0 16.0 15.8 14.012.6DS-Agent7.48.26.27.27.28.26.4 10.26.26.07.4 9.67.5GPT-4ResearchAgent7.6 8.6 10.6 11.8 10.0 9.4 12.67.2 10.4 10.0 10.69.29.8DS-Agent 3.4 4.2 5.8 4.4 4.4 4.4 5.4 6.66.85.6 4.4 4.45.0Best RankGPT-3.5ResearchAgent8.0 10.0 12.0 13.0 7.0 11.0 12.0 9.0 15.0 16.0 15.0 14.011.8DS-Agent5.02.02.03.03.0 6.01.07.02.01.02.0 6.03.3GPT-4ResearchAgent6.0 5.0 7.0 10.0 10.03.0 9.02.01.02.0 7.03.05.4DS-Agent 1.0 1.0 1.0 1.0 1.0 1.03.01.04.02.01.0 1.01.5
Figure 4.Success rate of four different agents in the developmentstage. The reported results are averaged across ﬁve repetitive trials.(Huang et al.,2023), which is the state-of-the-art languageagent for solving ML research related tasks. Both agents areimplemented on top of GPT-3.5 and GPT-4, respectively.Comparison on success rate.First, we analyze the successrate of different agents in terms of six different types ofdata science tasks in the development stage. As shownin Figure4, DS-Agent with GPT-4 achieves the highestsuccess rate of 100% over all the tasks. Notably, DS-Agentwith GPT-3.5 consistently surpasses ResearchAgent withGPT-4 in all tasks, underscoring the effectiveness of theproposed agent framework. Among them, ResearchAgentwith GPT-3.5 almost fails in every type of task, which can beattributed to its demanding requirements for the reasoningand coding abilities of LLMs. Interestingly, agents exhibita higher level of proﬁciency in tabular tasks compared toother types of tasks. This inclination can be explained by theobservation that tabular tasks usually entail simply callingfunctions from sklearn (Pedregosa et al.,2011), demandingconsiderably less reasoning and coding ability from LLMagents compared to other tasks.Table 2.Ablation results in terms of average best rank over 12development tasks. Results are reported over ﬁve repetitive trials.GPT-4 Average Best RankDS-Agent2.08DS-Agent w/o ReviseRank 2.58DS-Agent w/o CBR 3.41Comparison on task-speciﬁc evaluation metric.Next, weturn our attention to a detailed comparison based on task-speciﬁc evaluation metrics across 12 development tasks.The corresponding results are presented in Table1. Fromthe table, we can observe that DS-Agent with GPT-4 sig-niﬁcantly outperforms other agents in terms of both meanrank and best rank. Particularly, DS-Agent with GPT-4achieves the best performance in 9 out of 12 data sciencetasks. Furthermore, DS-Agent with GPT-3.5 achieves thesecond best average result in terms of both mean and bestrank, even surpassing ResearchAgent with GPT-4 across themajority of tasks. These ﬁndings highlight the superiorityof DS-Agent in solving data science tasks.A crucial aspect of DS-Agent’s design lies in the automaticiteration pipeline supported by CBR, allowing it to con-sistently revise the experiment plan by incorporating realfeedback from code execution. To illustrate this process,we depict the average best mean rank of DS-Agent acrossall tasks as iteration steps increase in Figure1(b). The no-ticeable performance improvement of DS-Agent with bothGPT-3.5 and GPT-4 over increasing iterations demonstratesthe efﬁcacy of the proposed automatic iteration pipeline.4.2.2. ABLATIONSTUDYTo validate the effectiveness of the CBR paradigm in thedevelopment stage, we conduct two ablation studies on DS-Agent, and the results are presented in Table2.Firstly, we investigate(1) w/o ReviseRank, which directlyutilizes the top-ranked retrieved case without adjusting theretrieved case based on execution feedback, which can be6

----- Page 7 -----

DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based ReasoningTable 3.Mean rank w.r.t. task-speciﬁc evaluation metric results on 18 data science tasks in the deployment stage. Results are reportedover 10 repetitive runs. Best performances are highlighted in bold, and second best performances are underlined.JS HR BPP WR DAG BQ TFC WTH ELE SRC UGL HB CA CS MH SS CO SDAvgMixtralZero-shot37.0 35.0 35.0 31.0 35.0 32.0 29.0 32.0 30.0 44.0 54.0 46.0 73.1 66.6 65.8 63.6 33.7 72.045.3-8x7bOne-shot35.2 35.0 32.2 31.0 35.0 29.1 29.0 32.0 30.0 36.5 47.1 46.0 50.1 53.1 51.2 51.1 23.6 61.539.4-InstructDS-Agent37.0 35.0 35.0 31.0 35.0 32.0 29.0 32.0 30.020.1 16.4 38.525.3 54.5 53.7 53.9 32.2 47.635.5GPT-3.5Zero-shot21.7 35.0 30.1 28.6 27.1 28.3 27.1 29.1 28.1 33.1 48.4 21.4 29.0 35.3 28.8 35.7 25.2 42.330.8One-shot27.6 25.8 27.6 25.6 34.6 23.0 20.8 29.1 27.0 35.7 48.421.127.1 50.5 58.4 57.5 33.9 56.435.0DS-Agent6.022.6 15.020.615.113.117.313.414.420.013.0 23.0 29.019.37.6 2.037.019.517.1GPT-4Zero-shot36.7 31.8 35.0 29.0 29.4 32.0 29.0 32.0 30.0 37.3 45.7 33.61.0 15.323.2 17.9 28.3 20.128.2One-shot35.1 24.413.826.6 29.6 28.8 23.1 30.1 26.6 26.7 41.6 36.7 29.7 21.9 35.3 28.921.4 23.228.0DS-Agent18.61.014.65.2 6.218.815.7 6.3 8.1 20.0 11.421.21.032.614.58.213.0 12.412.7
Figure 5.One pass rate of nine different agents over 18 deploymenttasks. The reported results are averaged across 10 random runs.also regarded as a RAG based LLM agent. As expected,this ablation leads to performance deterioration, indicatingthe importance of adjusting the retrieved case based on theexecution feedback in the retrieval process.Next, we evaluate(2) w/o CBRto verify the overall ef-fectiveness of the CBR paradigm, which prompts LLMsto generate experiment plans without incorporating humaninsights. This variant yields the worst performance amongthree agents, since LLMs are not trained to align with thedata science scenario, thus incapable of autonomously for-mulating reasonable experiment plans. Integrating the CBRparadigm successfully addresses this limitation, empower-ing LLMs to adeptly incorporate the expert knowledge fromKaggle to solve data science tasks.4.3. Results for Deployment Stage4.3.1. MAINRESULTSBaselines.In the deployment stage, we compare DS-Agent with two baselines:(1) Zero-shotdirectly promptsLLMs for code generation.(2) One-shotincorporatesa random example case from the agent case bank intothe context of LLMs. This can also be considered as anablation of the retrieval process. All agents are imple-mented using GPT-3.5, GPT-4, and an open-source LLMMixtral-8x7b-Instruct(Jiang et al.,2024).Comparison on one pass rate.First, we investigate the onepass rate of nine different agents over 18 deployment tasks.As depicted in Figure5, DS-Agent demonstrates remarkablesuperiority over alternative baselines across various LLMs.Particularly noteworthy is the outstanding performance ofDS-Agent with GPT-4, achieving an unprecedented one passrate of nearly 100%. Moreover, DS-Agent with GPT-3.5, at-tains the second-highest one pass rate at 85%. Furthermore,DS-Agent with Mixtral-8x7b-Instruct results in a notable25% improvement in one pass rate compared to the zero-shot strategy. These results highlight the efﬁcacy of theCBR paradigm in augmenting the bug-free programmingcapabilities of LLMs for data science tasks. While the one-shot strategy generally brings improvement compared tothe zero-shot approach, there is an exception in the case ofGPT-3.5, possibly attributed to its comparatively inferiorreasoning capabilities. Additionally, DS-Agent consistentlyoutperforms one-shot strategy, underscoring the importanceof the retrieval process.Comparison on task-speciﬁc evaluation metric.Then,our focus shifts to the task-speciﬁc performance of 18 de-ployment tasks, as outlined in Table3. Notably, DS-Agentwith GPT-4 attains the highest mean rank among the nineagents, while DS-Agent with GPT-3.5 secures the second-highest mean rank, even surpassing baselines with GPT-4.Unfortunately, DS-Agent with the open-source LLM stillexhibits weaker performance than agents with GPT-3.5 orGPT-4, attributable to its inferior foundational LLM capa-bilities. Nevertheless, it still outperforms or competes favor-ably with other baselines with the open-sourced LLM in 13out of 18 deployment tasks. These empirical observationssubstantiate the efﬁcacy of the proposed CBR paradigm.Comparison on resource cost.One crucial design of DS-Agent lies in its two distinct stages. The development stagefocuses on exploring effective model deigns, incurring rel-atively high resource cost, while the deployment stage istailored for swiftly and efﬁciently solving data science taskswith minimal resources. As shown in Table4, in the deploy-ment stage, DS-Agent incurs costs of$0.0045 and$0.1350for a single run with GPT-3.5 and GPT-4, respectively. Thisrepresents a substantial cost reduction of over 90% when7


----- Page 8 -----

DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning
(a)(b)Figure 6.Further analyses on DS-Agent in the deployment stage. (a) Performance difference of DS-Agent learning from past successfulexperiences or textual human insights. (b) Hyper-parameter study on varying number of example case in DS-Agent with GPT-3.5.Table 4.Monetary cost comparison among development and de-ployment stage on a single run.DS-AgentDevelopment Deployment Cost DeductionStage Stage PercentageGPT-3.5$0.06 $0.0045 92.5%GPT-4$1.60 $0.1350 91.5%compared to the development stage, rendering DS-Agenthighly appealing for real-world deployment scenarios.4.3.2. FURTHERANALYSESAblation study.In the development stage, DS-Agent adaptspast successful agent experiences to solve the unseen datascience tasks. One natural idea is to directly integrate thecollected textual human insights from the development stageinto the context of LLMs to enhance its data science capa-bilities. To this end, we investigate an ablation variant ofDS-Agent, which learns from relevant human insight forcode generation in the deployment stage. As shown inFigure6(a), DS-Agent that learns from past successfulexperiences signiﬁcantly outperforms its counterpart thatlearns from textual human insights across nearly all tasks.This demonstrates that learning from the homogeneous case(i.e., an example task and one of its solutions) leads to betterperformance than from the heterogeneous case (i.e., textualsolution insights). This ﬁnding emphasizes the crucial roleof both development and deployment stages in DS-Agent.Hyper-parameter analysis on case number in the context.Next, we delve into a crucial hyperparameter within DS-Agent: the number of retrieved example cases in the contextof LLMs, as illustrated in Figure6(b). Notably, DS-Agent,when devoid of example cases, regresses to the zero-shotstrategy, resulting in the poorest performance among all thesettings. This underscores the ability of LLMs to glean valu-able insights from contextual cases for solving ML tasks.Intriguingly, as the number of example cases in the con-text increases, the performance of DS-Agent experiences arapid decline—an unexpected outcome in typical few-shotlearning scenarios. It is essential to highlight that the reuseprocess in DS-Agent is centered around adapting a singleexample case to address the current ML task. Therefore,the presence of more than one example case in the contextintroduces interference information to LLMs, hindering itsability to generate appropriate code for the current task.5. Related WorkLLM Agent.LLMs have demonstrated remarkable founda-tional capabilities, such as language understanding, complexreasoning, tool usage, and code generation, which gives riseto the development of autonomous language agents designedfor various tasks (Yao et al.,2022;Hong et al.,2024b;Wuet al.,2023;Wang et al.,2023c;Zhao et al.,2024;Boikoet al.,2023;Romera-Paredes et al.,2023;Deng et al.,2023;Lin et al.,2023). Within the ﬁeld of data science,Ma-hadi Hassan et al.(2023) discusses the potential of LLMs asconversational agents in data science workﬂows. Moreover,recent studies have investigated the use of LLM agents indiverse areas such as feature engineering (Hollmann et al.,2023), hyper-parameter tuning (Zhang et al.,2023c;a), usingML libraries (Liu et al.,2023), aiding AI research (Huanget al.,2023), data operation (Lai et al.,2023), etc. In con-trast to them, we focus on developing automatic languageagents to build and train ML models, contributing to theﬁeld of automated data science. Concurrent to our work,Hong et al.(2024a) propose Data Interpreter, which focuseson optimizing the workﬂow of LLM agents for data sciencescenarios to fully unlock the intrinsic knowledge of LLMs.The core techniques of Data Interpreter and DS-Agent arecomplementary. A potential future work would be to en-hance DS-Agent with Data Interpreter or integrate CBR intoData Interpreter for further improvement.Case-Based Reasoning.Case-Based Reasoning (CBR)(Kolodner,1992;Watson & Marir,1994;Aamodt & Plaza,1994), a classical AI paradigm proposed several decadesago, aims to address new problems by adapting insightsfrom analyzing and reasoning with retrieved relevant cases.Integrating CBR into LLMs (Yang et al.,2023;Sourati et al.,2023;Guo et al.,2023) exhibits procedural similarities to thewell-known retrieval-augmented generation (RAG) frame-8

----- Page 9 -----

DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoningwork (Lewis et al.,2020;Rubin et al.,2022;Wang et al.,2023b;Gao et al.,2023), particularly in the steps of retrievaland reuse. However, a distinctive feature of CBR lies in itsfeedback mechanism, which enables iteratively adjustingthe retrieved cases and revising the solutions accordingly.Furthermore, CBR enhances future problem-solving by re-taining and reusing successful cases.6. ConclusionIn this work, we propose DS-Agent, a novel framework thatharnesses LLM agent and case-based reasoning to solvedata science tasks. In the development stage, DS-Agentstructures an automatic iteration pipeline on top of the CBRframework, which aims to retrieve and reuse relevant humaninsights from Kaggle to develop the experiment plan, andthen iteratively adjust the retrieved cases and revise the planbased on the execution feedback. As for the deploymentstage, DS-Agent leverages a simpliﬁed CBR framework toachieve a low-resource scenario by retrieving and reusingthe successful solution cases collected from the developmentstage. Extensive experiments are conducted to demonstratethe effectiveness of DS-Agent for data science tasks.AcknowledgementsWe truly thank the reviewers for their great effort inour submission. This work was supported by the Na-tional Key R&D Program of China under Grant (No.2023YFF0905400), National Natural Science Foundation ofChina through grants (No. U2341229, No. 61976102, No.U19A2065), the Key R&D Project of Jilin Province, China,(No. 20240304200SF), and the International CooperationProject of Jilin Province, China, (No. 20220402009GH).Impact StatementHere we emphasize some potential ethics concerns of DS-Agent:(1) Unemployment and skill obsolescence.Themajor concern of our research lies in potential unemploy-ment and skill obsolescence. However, as discussed in(Karmaker et al.,2021), the intent of automated data scienceis not to replace data scientists but rather to assist them, al-lowing them to concentrate on more complex aspects of datascience work. Wherein they only need to focus on higher-level data science problems, such as task formulation, datavisualization, cleaning and curation, prediction engineering,and result summary and recommendation. Moreover, byenabling interaction through natural language, automateddata science lowers the barrier to entry, facilitating a moreaccessible pathway for users to glean insights from data,thereby democratizing the ﬁeld of data science.(2) Ma-licious code generation.An often underappreciated yetcritical concern with the proliferation of automated data sci-ence tools like DS-Agent is the potential for generating codethat could be detrimental to computational devices or dataintegrity. As DS-Agent navigates the vast terrain of possiblesolutions to a given data problem, it may inadvertently pro-duce code that is inefﬁcient, vulnerable to exploitation, oreven directly harmful. While such issues were not observedin our experiments, it is prudent for users to review anycode produced by DS-Agent before execution. To enhancesecurity, we recommend running DS-Agent within a Dockercontainer, which provides a layer of isolation for the host’sﬁle system.(3) Data Privacy and Security.To protectdata privacy and security, DS-Agent is designed to operatelocally, negating the need to upload sensitive data. How-ever, when integrating API-based Large Language Models(LLMs) like GPT-3.5 or GPT-4, there is an inherent privacyrisk since these interactions typically involve transmittingdata to external servers. We advise users to carefully inspectany data sent in API prompts to prevent unintentional datadisclosures.ReferencesAamodt, A. and Plaza, E. Case-based reasoning: Foun-dational issues, methodological variations, and systemapproaches.AI communications, 7(1):39–59, 1994.Boiko, D. A., MacKnight, R., Kline, B., and Gomes, G. Au-tonomous chemical research with large language models.Nature, 624(7992):570–578, 2023.Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog,A., Ho, D., Ibarz, J., Irpan, A., Jang, E., Julian, R., et al.Do as i can, not as i say: Grounding language in roboticaffordances. InConference on Robot Learning, pp. 287–318. PMLR, 2023.Carta, T., Romac, C., Wolf, T., Lamprier, S., Sigaud, O.,and Oudeyer, P.-Y. Grounding large language models ininteractive environments with online reinforcement learn-ing. InProceedings of the 40th International Conferenceon Machine Learning, volume 202 ofProceedings of Ma-chine Learning Research, pp. 3676–3713. PMLR, 23–29Jul 2023.Chase, H. LangChain, October 2022. URLhttps://github.com/langchain-ai/langchain.Chen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan, K.,and Yao, S. Fireact: Toward language agent ﬁne-tuning.arXiv preprint arXiv:2310.05915, 2023.Christianos, F., Papoudakis, G., Zimmer, M., Coste, T., Wu,Z., Chen, J., Khandelwal, K., Doran, J., Feng, X., Liu, J.,et al. Pangu-agent: A ﬁne-tunable generalist agent withstructured reasoning.arXiv preprint arXiv:2312.14878,2023.9


----- Page 10 -----

DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based ReasoningDe Bie, T., De Raedt, L., Hern´andez-Orallo, J., Hoos, H. H.,Smyth, P., and Williams, C. K. Automating data science.Communications of the ACM, 65(3):76–87, 2022.Deng, C., Zhang, T., He, Z., Chen, Q., Shi, Y., Zhou,L., Fu, L., Zhang, W., Wang, X., Zhou, C., Lin, Z.,and He, J. K2: A foundation language model forgeoscience knowledge understanding and utilization.2023. URLhttps://api.semanticscholar.org/CorpusID:259108887.Erickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy,P., Li, M., and Smola, A. Autogluon-tabular: Robustand accurate automl for structured data.arXiv preprintarXiv:2003.06505, 2020.Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y.,Sun, J., and Wang, H. Retrieval-augmented generationfor large language models: A survey.arXiv preprintarXiv:2312.10997, 2023.Guo, C., Tian, Z., Tang, J., Wang, P., Wen, Z., Yang, K., andWang, T. A case-based reasoning framework for adaptiveprompting in cross-domain text-to-sql.arXiv preprintarXiv:2304.13301, 2023.Hollmann, N., M¨uller, S., and Hutter, F. Large languagemodels for automated data science: Introducing caafe forcontext-aware automated feature engineering. InThirty-seventh Conference on Neural Information ProcessingSystems, 2023.Hong, S., Lin, Y., Liu, B., Wu, B., Li, D., Chen, J., Zhang,J., Wang, J., Zhang, L., Zhuge, M., et al. Data inter-preter: An llm agent for data science.arXiv preprintarXiv:2402.18679, 2024a.Hong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang,C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., et al.MetaGPT: Meta programming for multi-agent collabo-rative framework. InThe Twelfth International Confer-ence on Learning Representations, 2024b. URLhttps://openreview.net/forum?id=VtmBAGCN7o.Huang, Q., Vora, J., Liang, P., and Leskovec, J. Benchmark-ing large language models as ai research agents.arXivpreprint arXiv:2310.03302, 2023.Hutter, F., Kotthoff, L., and Vanschoren, J.Automated ma-chine learning: methods, systems, challenges. SpringerNature, 2019.Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna,E. B., Bressand, F., et al. Mixtral of experts.arXivpreprint arXiv:2401.04088, 2024.Karmaker, S. K., Hassan, M. M., Smith, M. J., Xu, L., Zhai,C., and Veeramachaneni, K. Automl to date and beyond:Challenges and opportunities.ACM Computing Surveys(CSUR), 54(8):1–36, 2021.Kim, G., Baldi, P., and McAleer, S. M. Language modelscan solve computer tasks. InThirty-seventh Conferenceon Neural Information Processing Systems, 2023.Kolodner, J. L. An introduction to case-based reasoning.Artiﬁcial intelligence review, 6(1):3–34, 1992.Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efﬁcientmemory management for large language model servingwith pagedattention. InProceedings of the 29th Sym-posium on Operating Systems Principles, pp. 611–626,2023.Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettle-moyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu, T.Ds-1000: A natural and reliable benchmark for data sci-ence code generation. InInternational Conference onMachine Learning, pp. 18319–18345. PMLR, 2023.LeDell, E. and Poirier, S. H2o automl: Scalable automaticmachine learning. InProceedings of the AutoML Work-shop at ICML, volume 2020. ICML, 2020.Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,Goyal, N., K¨uttler, H., Lewis, M., Yih, W.-t., Rockt¨aschel,T., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neural Information Pro-cessing Systems, 33:9459–9474, 2020.Lin, Z., Deng, C., Zhou, L., Zhang, T., Xu, Y., Xu,Y., He, Z., Shi, Y., Dai, B., Song, Y., Zeng, B.,Chen, Q., Shi, T., Huang, T., Xu, Y., Wang, S., Fu,L., Zhang, W., He, J., Ma, C., Zhu, Y., Wang, X.,and Zhou, C. Geogalactica: A scientiﬁc large lan-guage model in geoscience.ArXiv, abs/2401.00434,2023. URLhttps://api.semanticscholar.org/CorpusID:266693296.Liu, Y., Tang, X., Cai, Z., Lu, J., Zhang, Y., Shao, Y., Deng,Z., Hu, H., Yang, Z., An, K., et al. Ml-bench: Large lan-guage models leverage open-source libraries for machinelearning tasks.arXiv preprint arXiv:2311.09835, 2023.Mahadi Hassan, M., Knipper, A., and Kanti Karmaker Santu,S. Chatgpt as your personal data scientist.arXiv e-prints,pp. arXiv–2305, 2023.OpenAI. Introducing chatgpt. 2022. URLhttps://openai.com/blog/chatgpt.OpenAI. Gpt-4 technical report.arXiv preprintarXiv:2303.08774, 2023.10