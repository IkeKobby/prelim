TITLE: MiniCPM — Unveiling the Potential of Small Language Models with Scalable Training Strategies
FORMAT: Plain text extracted from PDF with LaTeX placeholders for figures/tables/algorithms.
NOTE: Replace each placeholder with the correct diagram or LaTeX table.
-----

MiniCPM: Unveiling the Potential of Small Language Modelswith Scalable Training StrategiesShengding Hu1, Yuge Tu2, Xu Han1→, Chaoqun He1, Ganqu Cui1, Xiang Long2,Zhi Zheng2, Yewei Fang2, Yuxiang Huang1, Weilin Zhao1, Xinrong Zhang1,Zheng Leng Thai1,Kaihuo Zhang2, Chongyi Wang2, Yuan Yao1,Chenyang Zhao1, Jie Zhou2, Jie Cai2, , Zhongwu Zhai2, Ning Ding1,Chao Jia2, Guoyang Zeng2, Dahai Li2, Zhiyuan Liu1*, Maosong Sun1*1Department of Computer Science and Technology, Tsinghua University.2Modelbest Inc.shengdinghu@gmail.comAbstractThe burgeoning interest in developing Large Language Models (LLMs) withup to trillion parameters has been met with concerns regarding resourceefﬁciency and practical expense, particularly given the immense cost ofexperimentation. This scenario underscores the importance of exploringthe potential of Small Language Models (SLMs) as a resource-efﬁcientalternative. In this context, we introduce MiniCPM, speciﬁcally the 1.2B and2.4B non-embedding parameter variants, not only excel in their respectivecategories but also demonstrate capabilities on par with 7B-13B LLMs.While focusing on SLMs, our approach exhibits scalability in both modeland data dimensions for future LLM research. Regarding model scaling, weemploy extensive model wind tunnel experiments for stable and optimalscaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD)learning rate scheduler (LRS), conducive to continuous training and domainadaptation. We present an in-depth analysis of the intriguing trainingdynamics that occurred in the WSD LRS. With WSD LRS, we are now ableto efﬁciently study data-model scaling law without extensive retrainingexperiments on both axes of model and data, from which we derive themuch higher compute optimal data-model ratio than Chinchilla Optimal.Additionally, we introduce MiniCPM family, including MiniCPM-DPO,MiniCPM-MoE and MiniCPM-128K, whose excellent performance furthercementing MiniCPM’s foundation in diverse SLM applications. MiniCPMmodels are available publicly1.1 IntroductionFollowing the revelation of the scaling law (Kaplan et al.,2020), there has been a vigorouspursuit in the ﬁeld of Large Language Models (LLMs) (Hoffmann et al.,2022;Bai et al.,2023;Gemini et al.,2023;Chowdhery et al.,2023;Achiam et al.,2023), encompassing models withup to an astonishing number of parameters in the trillions (Fedus et al.,2022). These modelshave emerged as a pivotal driving force in the evolution of artiﬁcial intelligence.Nonetheless, the training of such large-scale models is both ﬁnancially burdensome andoperationally inefﬁcient. On one hand, the empirical understanding of the mechanismsunderpinning the training of LLMs remains elusive. Given the signiﬁcant economic andenvironmental costs, experiments on LLMs are prohibitively expensive for most researchersand corporations. On the other hand, the deployment of these colossal models in everydayscenarios, such as on personal computers or smartphones, is either inefﬁcient or unfeasible.→Corresponding Authors.1https://github.com/OpenBMB/MiniCPM1arXiv:2404.06395v3  [cs.CL]  3 Jun 2024

Both aspects underscore the imperative to refocus efforts on comprehensively exploringsmaller, yet potent, language models (SLMs). These models on the one hand provide efﬁcientsolutions to practical deployment, on the other hand, if trained with scalable strategies, theycan potentially guide the development of future larger models.Recently, a resurgence of interest has been observed in the domain of SLMs, evidenced bythe advent of a series of innovative models such as the Phi series (Gunasekar et al.,2023;Liet al.,2023b;Javaheripi & Bubeck,2023), TinyLlama (Zhang et al.,2024a), MobileLLM (Liuet al.,2024), and Gemma (Banks & Warkentin,2024), among others. While these models havesigniﬁcantly contributed to the expansion and diversiﬁcation of the SLM landscape, thereremain two pivotal areas where these models have yet to fully satisfy prevailing interests:(1) the development of comprehensive abilities akin to those exhibited by LLMs; and (2) theformulation of transparent and scalable training methodologies that could further propelthe evolution of both SLMs and LLMs.In this paper, we introduce MiniCPM, a series of SLMs, which primarily builds on twomodels, endowed with 2.4B and 1.2B non-embedding parameters respectively, and theyrank preeminently in their respective 2B and 1B scale categories. MiniCPM also exhibitscomparable capabilities to those of 7B↑13B language models, such as Llama2-7B (Touvronet al.,2023), Mistral-7B (Jiang et al.,2023), Gemma-7B (Banks & Warkentin,2024), andLlama-13B (Touvron et al.,2023), etc. Notwithstanding their small model sizes, our trainingmethodology is meticulously designed to facilitate seamless scaling of both model scaleand data horizons. This is exempliﬁed through our model wind tunnel experiments thatencompass comprehensive hyper-parameter optimization (Section3), and the deploymentof a WSD (Warmup-Stable-Decay) learning rate scheduler (Section4). The latter is tailoredfor continuous training with an un-predeﬁned pre-training token number and makes thereusing of model intermediate checkpoints highly feasible. A detailed analysis of the trainingdynamics of MiniCPM is presented, suggesting that the WSD scheduler demonstrates theintriguing loss landscape of model pre-training. With the WSD scheduler, we are now alsocapable of studying the data-model scaling law with linear effort on the model axis and anegligible effort on the data axis, while the traditional ones need quadratic effort consideringthe scaling along both model and data axes. The result of the scaling law indicates a muchhigher data size/model size ratio compared with Chinchilla Optimal (Hoffmann et al.,2022).Moreover, we introduce the MiniCPM family, including MiniCPM-DPO, MiniCPM-128K,and MiniCPM-MoE. We conduct evaluations of the MiniCPM family against establishedbenchmarks and illuminate their impressive capabilities as SLMs: (1) The foundation modelssurpass Mistral-7B, and LLama-13B. (2) The DPO model surpasses zephyr-7B (Tunstall et al.,2023) on MTBench (Zheng et al.,2024) (3) The 2.4B MiniCPM-128K model demonstratesperformance either surpassing or matching that of models like Yarn-Mistral-7B-128K (Penget al.,2023) and ChatGLM3-6B-128K (Du et al.,2021). (4) The MiniCPM-MoE, with 4Bactivated parameters, is on par with Llama2-34B (Touvron et al.,2023).In summary, MiniCPM propounds a new stage in the development of small languagemodels, exemplifying the latent potential within SLMs and advocating for a more scientiﬁcand sustainable approach toward scaling up LLMs.2 Related WorkSmall Language Models.“Small Language Models” (SLMs) is an evolving concept that hasundergone signiﬁcant transformations over time. Presently, SLMs are generally construed asmodels that are smaller in scale compared to the well-known LLMs, typically not exceeding7 billion parameters. These models are distinguished by their capacity for deploymenton end-user devices, such as personal computers and smartphones, even in the absenceof a GPU. Notable examples within the current landscape of SLMs include the Phi series(Gunasekar et al.,2023;Li et al.,2023b;Javaheripi & Bubeck,2023), TinyLlama (Zhanget al.,2024a), MobileLLM (Liu et al.,2024), and Gemma (Banks & Warkentin,2024), etc.A variety of methodologies have been explored to augment the efﬁcacy of SLMs. Theseinclude the incorporation of high-quality data (Gunasekar et al.,2023;Li et al.,2023b;Javaheripi & Bubeck,2023), the application of structure pruning techniques (Xia et al.,2023),and the reconﬁguration of model architectures (Liu et al.,2024), among others. MiniCPM2

enhances the capabilities of SLMs through a meticulous amalgamation of hyper-parameteroptimization, strategic training methodologies, architectural design, and high-quality data.Scalable Pre-training Strategies.Since the discovery of scaling law (Kaplan et al.,2020;Raeet al.,2021;Aghajanyan et al.,2023), scientiﬁcally and predictably (Achiam et al.,2023;Huet al.,2023;Du et al.,2024) scaling up the LLMs has been pursued from diverse perspectives,especially for the pre-training stage. In terms of training stability, the Tensor Programseries (Yang et al.,2022;2023) is introduced to ensure optimal hyper-parameter consistencyacross varying model scales, a technique employed in training CerebrasGPT (Dey et al.,2023). Furthermore,Wortsman et al.(2023) suggest leveraging smaller models to anticipateand mitigate instabilities in larger model training. From the training data standpoint,various data-centric strategies have been advocated (Xie et al.,2024;Shi et al.,2023;Yeet al.,2024). In the realm of training methodologies, prior research has delved into diverselearning rate schedulers (LRS) (Howard & Ruder,2018;Raffel et al.,2020;Hundt et al.,2019),with the Cosine LRS (Loshchilov & Hutter,2016) emerging as the predominant choice inLLMs.Kaplan et al.(2020) andHoffmann et al.(2022) have meticulously examined thehyper-parameters of Cosine LRS, thereby laying a foundational groundwork for subsequentpre-training works. Of these, DeepSeek (Bi et al.,2024) bears the closest resemblance to ourproposed WSD LRS. Concerning batch size scheduling,Smith et al.(2017) advocates forincrementing batch size as an alternative to diminishing learning rate, a strategy recentlyadopted by Yi-9B (Young et al.,2024).3 Model Wind Tunnel ExperimentsAlthough we target at training SLMs that can be quickly deployed onto end devices, weenvision that many aspects of model training are universal across scales. Extensive experi-ments should be conducted through an SLM to explore the limit of SLM before transferringthe experience into LLMs. These experiments take the spirit of wind tunnel testing indeveloping an aircraft, thus we name it Model Wind Tunnel Experiments (MWTE). In thispaper, the MWTE contains three parts: (1) Hyper-parameters; (2) Optimal Batch-size Scaling;and (3) Optimal Learning Rate Stability.3.1 Scaling Hyper-parameters Invariant LMHyper-parameters have a signiﬁcant impact on the performance of a model. However,adjusting hyper-parameters for each model in traditional training is not feasible for LLMs.Even for SLM like MiniCPM, extensive experiments on hyper-parameters search take a lotof resources. Tensor Program (Yang et al.,2022;2023) proposes a framework to stabilize thehyper-parameters for models with different scales. The main part of the Tensor Programis the width scaling (Yang et al.,2022) and the depth scaling (Yang et al.,2023). Theformer technique supports CerebrasGPT (Dey et al.,2023) to predict the loss of LLMs moreaccurately. In MiniCPM, we use both two scaling techniques. The speciﬁc scaling operationsare listed in Table7. We do not apply the attention softmax scaling techniques (Yanget al.,2022). DespiteYang et al.(2023) observing that depth scaling for a network withblock depth larger than two is not satisfying, we ﬁnd the resulting optimal learning rate isstable empirically. Details of the hyper-parameters and Tensor Program Operations are inAppendixA.1.3.2 Optimal Batch SizeBatch size determines the balance between the convergence speed of the model and theconsumption of computational resources. If the batch size is too large, it will result in asigniﬁcant amount of data and computation costs. On the other hand, if the batch sizeis too small, it will require a large number of training steps and may result in a limiteddecrease in the loss function. We followKaplan et al.(2020) to determine the batchsize fromexpected loss, with a slight modiﬁcation from their setting (see AppendixA.2). We conductexperiments on 0.009B, 0.03B, and 0.17B models, respectively, toward this goal. Each modelsize is trained on 6 batch sizes with a global learning rate of 0.01 and cosine learning rate3

scheduler. We observe the trend of the optimal batch size with loss on the C4 (Raffel et al.,2019) dataset (red line in the Figure1). Figure 1: We demonstrate the loss curve of three size modelstrained using different batch sizes. Each vertical line formedby points with a gradient color represents a training curve.Lighter colors denote higher loss. Figure 2: The connected op-timal batch sizes.As shown in Figure1, we plot the batch size in the x-axis, and token consumption in they-axis, the color of the points represents a loss. Thus a horizontal line formed by the colorpoints denotes a training curve. we use parabolas to ﬁt the equal-loss points and connectthe minima of the parabolas with red lines. The lines demonstrate the optimal batch sizeshifts large as the loss decreases. We then connect the three lines (see Figure2) and ﬁnd thatthe lines connect each other well into a linear relationship in the log space, from which weobtain the following relationship between batch sizebsand C4 LossL:bs=1.21↓109L6.24.W enote that it might seem strange that the batch size should be estimated from a rough lossprediction that we can only have after training. We provide our comment in AppendixA.2.3.3 Optimal Learning RateDue to our use of Tensor Program (Yang et al.,2022;2023), we anticipate that the learningrate, will not undergo signiﬁcant changes during model scaling. To verify this, we conductsix sets of learning rate experiments at 0.04B, 0.1B, 0.3B, and 0.5B. In Figure3, we ﬁnd thatalthough the model size has increased by ten times, the optimal base learning rate2does notshow a noticeable shift and remains around 0.01. We further conduct a simple validation ona scale of 2.1B and conﬁrm that a learning rate of 0.01 indeed achieves the lowest loss. Figure 3: Loss vs Learning Rate. After ap-plying for the Tensor Program, the learn-ing rate shift becomes minimal.Figure 4: Cosine Learning Rate Schedulerwith different periods. The Y-axis is theloss on the C4 corpus.4 WSD Learning Rate Scheduler4.1 Analysing Cosine LRSThe learning rate scheduler (LRS), which adjusts the learning rate used in different stagesof training, is crucial for model performance. The current commonly used learning rate2The actual learning rate of 2-D tensors will be scaled according to Tensor Program.4

strategy is the Cosine LRS (Kaplan et al.,2020;Hoffmann et al.,2022;Rae et al.,2021;Touvronet al.,2023;Bai et al.,2023;Almazrouei et al.,2023), which gradually decreases the learningrate following a cosine curve after it reaches its maximum after the warmup stage.A key hyper-parameter in the Cosine LRS is the stepTat which Cosine decreases to theminimum for the ﬁrst time. Typically,Tis set to the total training stepSfor training with apredeﬁned training step. Generally, it is believed that the learning rate should be high toenable sufﬁcient exploration. For example,Kaplan et al.(2020) demonstrate that the lossdecreases when the summed learning rate over the entire training increases (see Figure 22in their paper). This indicates settingT<Sis not optimal. On the other hand,Hoffmannet al.(2022) make a key observation that settingT>Sresults in dropped performancewhile settingS=Tresults in improved training efﬁciency, conﬁrming that the learningrate shouldn’t be kept high throughout the training. To reproduce these observations,we conduct experiments on the 0.036B model. We tryCosine(T)andCosineLoop(T)LRS,following the formula shown in AppendixB.1. The result can be seen in Figure4. We can seethat when the training step isS=20N, 40N, 60N, 80N, the lowest loss is always achievedby theCosine(T)whereT=S. BothT<SandT>Sare not optimal.We hypothesize that the Cosine LR performs exceptionally well whenT=Sbecause of thefollowing two reasons: (1) Cosine LRS withT=Shas a longer duration ofhigh learningratetraining compared toT<Sand other LRS such as Linear LRS. This high learning ratemight help the model ﬁnd a better global optimum. (2) Cosine LRS withT=Shas a morethorough learning rate decay phase compared to Cosine LRS withT>Sand Constant LRS.This learning rate decay may involve unique training dynamics that enable the model toﬁnd a better local optimum.4.2 WSD LRSIn light of the above perspective, we propose to explicitly divide the training stage into thehigh learning rate stage and learning rate decay stage. We name it as the Warmup-Stable-Decay (WSD) LRS. Especially, the WSD LRS contains three stages: the warmup stage (whoseend step is denoted byW), the stable training stage (whose end step is denoted byT), andthe remaining decay stage. The function form of WSD is:WSD(T;s)=sWη,s<Wη,W<s<Tf(s↔T)η,T<s<S(1)where 0<f(s↔T)↗1 is a decreasing function abouts,ηis the maximum learning rate.Typically, as long as the warmup stage is enough, it affects little performance, therefore, weomitWin the subsequent discussion. With an abuse of notation, we will denote WSD witha clear stop point 20N40N60N80N100N120N140NTokens (B)3.63.84.04.24.4Loss on C4WSD(40N,2N)WSD(60N,2N)WSD(80N,2N)WSD(40N,4N)WSD(60N,6N)WSD(80N,8N)Cosine(80N)Figure 5: Model training loss has a suddendecrease in the decay stage of WSD LRS.0.00.51.01.52.02.5Compute⇥10193.43.63.84.04.24.4Loss on C4 3.32WSD(20N,2N)WSD(40N,4N)WSD(60N,6N)WSD(80N,8N)WSD(160N,16N)WSD(320N,32N)0.17B, WSD(40N,4N)Optimal Loss EnvelopeFigure 6: Continous training a 0.036B modelcan match the performance of 0.17B modelwith an acceptable increase in training com-pute.5

4.3 ExperimentsNext, we present several experimental ﬁndings of WSD LRS.Loss Decreases Dramatically in Decay Stage.We try WSD LRS on 0.036B models. Asshown in Figure5, in the decay stage, as the learning rate begins to decrease, the lossexperiences a signiﬁcant rapid decline and quickly decreases to be equal to or lower thanthe Cosine LRS at stepT=S. At the same time, we can reuse the model before decay andcontinue training with the previous high learning rate. After more steps of trainingS↘, wecan also perform annealing to achieve the same loss as the Cosine LRS atCosine(S↘). Thisveriﬁes our assumption that the training stage can be explicitly split into the stable trainingand decay stages.10% Steps are Enough.From the two-stage training perspective, shortening the decay stagewill greatly beneﬁt the fast test of different model checkpoints of stable training. Therefore,we conduct experiments that start from the same stable training checkpoints and havedifferent decay steps. Also shown in Figure5, among all three stable training checkpoints in40N, 60N, and 80N training data, having a decay of 10% of the total tokens is sufﬁcient toachieve the best results, while a decay of 2.5% of total tokens falls short. Therefore, in thesubsequent training experiments, we use a decay of about 10% to ensure full convergence.Effective Data Scaling with WSD LRS.With WSD LRS, we can continuously train the LMto extreme convergence. To further demonstrate the potential of training a ﬁxed-sized modelto convergence, we compare continuously training a 0.036B LM with a 0.17B model with40N data. In Figure6, the green lines represent 0.036B models trained with different stabletraining tokens. Despite the last point of the 0.036B series being trained with many moretokens than Chinchilla Optimal (Hoffmann et al.,2022), it still has space for performanceimprovement.To ﬁnd the limit of continuously training this ﬁxed-sized LM, we estimate how the model’soptimal performance changes with its computation during continuous training. By optimalperformance, we mean the loss of training tokenDis achieved byWSD(D, 0.1D). With aseries ofD, the losses will form the optimal loss envelope. Due to uncertainty about thefunction form of the loss envelope, we try two ﬁtting formulas: (1) exponential:L(C)=αe↔εC+L0and (2) power-law:L(C)=εC↔α+L0. The ﬁtting results for both functionsare in AppendixB.2. We ﬁnd that the power-law form ﬁts better (similar to the CosineLRS (Kaplan et al.,2020)). In Figure6, the ﬁtted curve is shown ingreendotted lines. Tointuitively estimate and comprehend the effect of continuous training such a ﬁxed-sizedmodel, we also trained a 0.17B model withWSD(40N,4N), which is shown inpinkinFigure6. We can see that a 0.036B model can match the performance of a 0.17B modelwith an acceptable increase (↑4 times) in training compute while saving a lot of inferencecomputation (Sardana & Frankle,2023) (saving↑5 times per inference call), indicating abetter inference-compute-optimal setting (Sardana & Frankle,2023).4.4 Analysis of the Decay StageIn this section, we provide a brief analysis of the loss drop in the decay stage, examiningit through the prisms of checkpoint updates and gradient information. We calculate themaximum weight element updatemaxij(W(t+1)ij↔W(t)ij)across all weight matrices in theMiniCPM-2.4B (Introduced in Section6). As depicted in Figure7, the updates exhibit arobust correlation with the learning rate’s magnitude. Notwithstanding the illustration ofthe two submodules (gateproj and qproj module of the 25th layer), this pattern is prevalentacross every layer and submodule within the network. This observation may not be trivial:the model checkpoints experience signiﬁcant updates preceding the learning rate’s decay,yet the loss exhibits minimal reduction. Conversely, during the decay stage, despite lesspronounced weight alterations, there is an accelerated decrease in loss.Further examination of the gradient data is undertaken by training a 0.2B model, meticu-lously recording every step gradient information, and evaluating the differences betweenconsecutive steps, thereby providing an approximation of second-order gradient infor-6

Figure 7: Max Difference of Checkpoints.mation. We treat the gradient at steptas a ﬂattened vectorg(t), and the parameter (alsoﬂattened as a vectorx(t)) update between steptandt+1 isv(t)=x(t+1)↔x(t). Thegradient norm take theL2 norm of the gradient≃g(t)≃, gradient inner product isg(t+1)·g(t),the cosine of the gradient’s angle is given byg(t+1)·g(t)≃g(t+1)≃≃g(t)≃. Imaging the optimization processas a trajectory over a high-dimension manifold, ﬁrst order directional derivative along thetrajectory is computed asD1=g(t+1)·v(t)≃v(t)≃, and the second order directional derivative isD2=(g(t+1)↔g(t))·v(t)≃v(t)≃2.D1,D2enables an approximate estimation of the loss curvature on thetrajectory,K=|D2|(1+D21)32. The results of these statistics over time are shown in Figure8.W ecan see that the gradient norm diminishes during the decay phase, and upon commence-ment of this stage, the cosine between gradients predominantly assumes positive values,suggesting that in the decay phase, model parameters undergo consistent changes acrosssteps. Concerning directional derivatives, it is remarkable that the ﬁrst-order directionalderivative diminishes exponentially with each step, aligning closely with the learning rate,while the second-order directional derivative exhibits a slight increase in magnitude. Thecurvature of the loss function also increases by a magnitude, indicating the proximity to alocal optimum. These ﬁndings potentially offer a deeper insight into the shape optimizationspace, a subject reserved for future exploration. 0200040006000800010000Iteration0.00.10.20.30.40.5maximum gradient 0200040006000800010000Iteration0.00.20.40.60.81.0gradient norm 0200040006000800010000Iteration 1.0 0.50.00.51.0cosine value of gradient’s angle 0200040006000800010000Iteration 0.010 0.0050.0000.0050.0102nd order directional derivative 0200040006000800010000Iteration10 310 210 1100negative 1st order directional derivative (log scale) 0200040006000800010000Iteration10 410 210 1loss curvature on parameter trajectory (log scale)Figure 8: Gradient statistics over the training of a 0.2B model using WSD LRS. The exponen-tial decay stage begins at 8000 steps.4.5 Measuring the Scaling Law with WSD LRSScaling laws serve as a fundamental guiding principle in the development of LLMs. Al-though these scaling laws exhibit variability in speciﬁc coefﬁcients due to diverse conﬁgura-tions across model series, the compute optimal data-to-model ratio remains a meaningful7

[FIGURE PLACEHOLDER]
\includegraphics[width=\linewidth]{figure_7_placeholder}


metric across different scaling law functions, which “marginalizes“ out the speciﬁc valueof loss. Regarding this ratio,Kaplan et al.(2020) posit that a tenfold increase in modelscale should equate to a singlefold increase in data scale. Conversely,Hoffmann et al.(2022) argue for the same scaling rate between model size and data size. What’s more,current models such as LLama 2 (Touvron et al.,2023), train much more data than whatHoffmann et al.(2022) claims, still yielding considerable performance gain. Indicating ahigher data-to-model ratio.This unaddressed uncertainty stems from the challenges inherent in training multiplemodels of varying sizes and data sizes in traditional scaling experiments. Previously, if theaverage cost of training one model size on one data size isC, then conducting the scalingexperiments withmmodel sizes andmdata sizes takes approximatelyO(m2)C.In this section, we introduce the utilization of the WSD scheduler as an effective approachto explore the scaling law with linear cost (O(mC)). Since the WSD scheduler has theadvantage of arriving at the optimal loss of Cosine LRS after decaying from stable stagecheckpoints of any step, we are now able to precisely measure the optimal scaling propertieswithout re-training the models from scratch to different amounts of tokens, thus making thescaling law measurement much more efﬁcient along the data axis.We measure the scaling law along the data and model axes by training SLMs of 6 sizesranging from 0.04B to 2B, each with 6 decayed models starting from the checkpoint of10Nto 60Ndata during the stable training stage. The ﬁnal loss is evaluated on ﬁve held-out evaluation datasets. To potentially compare the loss when the model uses differenttokenizers, we take the average of loss by a number of bytes instead of a number of tokens,followingAchiam et al.(2023). The ﬁnal loss of each pair of data size and model size isshown in the blue lines in Figure17.Then we ﬁt the losses with model sizeNand data sizeDfollowingHoffmann et al.(2022)using scipycurvefitfunction:L(N,D)=CNN↔α+CDD↔ε+L0(2)The ﬁtted curve along the data axis for each dataset and each checkpoint are shown inorange lines in Figure17. Then we have the optimal model sizeNopt, dataset sizeDopt,given a ﬁxed amount of computeC=6ND(Rae et al.,2021) as:NoptDopt=K2/parenleftbiggC6/parenrightbiggη, (3)whereK=(αCNεCD)1α+ε, andη=ε↔αα+ε. The derivation ofNoptclosely followsHoffmann et al.(2022) by substitutingDwithC6Nin Equation2, and minimizeL(N)givenC. A similar wayis adopted forDopt. From Equation3, whenα=ε,Nopt/Doptis a constant, supportingHoffmann et al.(2022)’s claim, and whenα<ε, we should emphasize more on parameterscaling (Kaplan et al.,2020), and vise versa.In our experiments, the ﬁtted relationship between loss andN,Dis shown in the contourplot of equal loss in Figure10. The equation of ﬁtted scaling law is shown in the ﬁrst text boxin each subplot. We can see that in all evaluation corpora, we haveε<α. More speciﬁcally,on average, we haveα=0.29,ε=0.23,K2=0.01,η=↔0.10. Sinceαis slightly larger thanε, this result shows that as the computation scale, we should slightly emphasize more ondata scaling than model scaling, which aligns withHoffmann et al.(2022).As for the concrete data-to-model ratioDoptNopt, we notice that there is a huge gap in computeoptimal regime between ours andHoffmann et al.(2022) despite that the trend ofDoptNopt’ withcomputeCis aligned between ours and theirs. Speciﬁcally, the data size should be 192 timeslarger than the model size on average, as opposed to 20 times inHoffmann et al.(2022). Wenote that this aligns with the observation in Section4.3and Figure6.8

With respect to the large deviation from Chinchilla OptimalNoptDopt, we notice that their scalingexperiment was conducted in a not very recent conﬁguration. To compare with more recentconﬁguration such as Llama2 (Touvron et al.,2023), we extract the training loss data fromLlama2 paper (left part) in Appendix Figure18and estimate the compute optimalDoptNoptintheir paper using the right part of Figure18. Since they use Cosine LRS, the loss is notoptimal in the middle of the training, depicted by the concave curve during training inthe right ﬁgure of Figure18. We ﬁll the concave part with a straight line to estimate theoptimal loss envelope if they had used the WSD LRS. After that, the compute model sizeshould roughly be the regime in which a model’s loss curve is about to intersect with alarger model’s loss curve. With this intuition, the 13B model is about to intersect with the34B model at 105EFlops (1018 Flops), and the 34B model is about to intersect with the 70Bmodel at 5↓105EFlops. Therefore, we estimate theDoptNoptto be roughly5↓1056↓342↑1056↓132, whichis 70↑100. Therefore, under this approximate comparison, their data-model ratio is closerto ours. And our conﬁguration can absorb more data into a smaller model compared toprevious ones. However, we note that the above estimates are only a rough one.A larger data-to-model ratio means that we can absorb more data into a smaller model thanwe previously thought, which is more efﬁcient for inference and deployment. We hope WSDLRS will help more researchers exploreL(N,D)with less effort and make the relationshipclearer in LLMs.5 Two Stage Pre-training StrategyTypically, the training of instruction following LLMs contains the pre-training stage andthe supervised ﬁne-tuning (SFT) stage (Zhang et al.,2023;Wei et al.,2021). In the pre-training stage, the data is composed of large-scale unlabeled data, while in the SFT stage,high-quality labeled data becomes the optimization target. In light of the pronounced lossdecrease observed during the decay stage of the WSD LRS, we postulate that the integrationof high-quality labeled data in this phase presents dual advantages:•Introducing this data during the annealing phase, in addition to the SFT stage,fosters a more comprehensive model learning. Speciﬁcally, it facilitates a morepronounced loss reduction in relation to the SFT data distribution, rather than thepre-training data distribution. This approach is more congruent with actual userscenarios.•In contrast to a uniform distribution of high-quality data throughout the entirepre-training process, this method enhances training by concentrating on data andsustaining continuous pre-training. If we do not predetermine a training step, wewill repeat a small dataset throughout an ongoing pre-training process, which couldlead to negative effects.Based on these two hypotheses, we propose the following training strategy: during the pre-training phase, only use large-scale coarse-quality pre-training data, which is abundant andcan support continuous training when provided with more computational resources. Duringthe annealing phase, we use diverse and high-quality knowledge and ability-oriented SFTdata, mixed into the pre-training data.To validate the advantages of our training strategy, we conduct comparison experimentsusing (A) MiniCPM-2.4B’s intermediate checkpoint in the stable stage; and (B) MiniCPM-1.2B’s last checkpoints in the stable stage. Speciﬁcally, we compare the following:1.A-1: 2.4B model, decay using only pre-training data, followed by 4B token SFT.2.A-2: 2.4B model, decay using the aforementioned high-quality data unlabeled dataand SFT data mixed into pre-training data, also followed by 4B token SFT.3.B-1: 1.2B model, decay using only pre-training data, followed by 6B token SFT.4.B-2: 1.2B model, decay using only pre-training data, followed by 12B token SFT.9

10 11001011021033⇥10 14⇥10 16⇥10 1Code 10 11001011021035⇥10 16⇥10 17⇥10 18⇥10 1English (Wikihow)10 1100101102103100 6⇥10 17⇥10 18⇥10 19⇥10 1Chinese (Wikihow) 10 1100101102103Compute4⇥10 16⇥10 1Ultratext10 1100101102103Compute100 7⇥10 18⇥10 19⇥10 1Chinese (Yayi Corpus)10 1100101102103Compute5⇥10 16⇥10 17⇥10 18⇥10 19⇥10 1Average 10 210 1100101102103104105106100 2⇥10 13⇥10 14⇥10 16⇥10 1Code 10 210 1100101102103104105106100 4⇥10 16⇥10 1English (Wikihow)10 210 1100101102103104105106100 6⇥10 1Chinese (Wikihow) 10 210 1100101102103104105106Compute100 3⇥10 14⇥10 16⇥10 1Ultratext10 210 1100101102103104105106Compute100 6⇥10 1Chinese (Yayi Corpus)10 210 1100101102103104105106Compute100 4⇥10 16⇥10 1Average0.031B0.11B0.25B0.49B0.85B2.0BReal Loss w.r.t. Compute Fitted Loss w.r.t. Compute Figure 9: The result of scaling experiments with WSD Scheduler (above) and the ﬁttedscaling curve (below). The x-axis is the computation FlopsC=6ND, each color of theline represents the same model with different computation Flops. We can see that smallermodels are better than larger models when the Flops are small and worse when the Flopsare large. Thus models of different sizes will intersect with each other in the plot around thecompute optimal regime.5.B-3: 1.2B model, annealing using the aforementioned high-quality data + SFT datamixed into pre-training data, also followed by 6B token SFT.The results of the experiments are shown in Table1. We can see that, despite the A-2 andA-1 have undergone the same SFT distribution, adding SFT data to the decay stage pushesthe boundary . Comparison between B-2 and B-3 demonstrate that the deﬁciency of onlySFT is not due to the insufﬁcient training tokens in SFT stage.The results indicate that the beneﬁts of introducing high-quality data at the beginning of thedecay stage are much higher than simply adding it during the SFT phase. Therefore, werecommend that specialization and enhancement of model capabilities should start fromthe decay phase.6 ModelIn this section, we begin to introduce the MiniCPM model that aggregates the aforemen-tioned observations and techniques.10

10 110010110 1100101102103104Compute (1018Flops)3.32⇥10 2N0.37+2.17⇥10 1D0.34+0.17K2=0.01 = 0.05DoptNopt  C=1021= 194.93Code 10 110010110 1100101102103104 8.08⇥10 2N0.26+2.97⇥10 1D0.18+0.27K2=0.02 = 0.18DoptNopt  C=1021= 166.04English (Wikihow) 10 110010110 1100101102103104 5.14⇥10 2N0.35+3.90⇥10 1D0.18+0.40K2=0.01 = 0.33DoptNopt  C=1021= 833.95Chinese (Wikihow) 10 1100101Non-embedding Parameters (109)10 1100101102103104Compute (1018Flops)7.54⇥10 2N0.30+2.92⇥10 1D0.30+0.25K2=0.01 = 0.00DoptNopt  C=1021= 95.60Ultratext 10 1100101Non-embedding Parameters (109)10 1100101102103104 1.53⇥10 1N0.19+3.76⇥10 1D0.17+0.35K2=0.01 = 0.05DoptNopt  C=1021= 112.79Chinese (Yayi Corpus) 10 1100101Non-embedding Parameters (109)10 1100101102103104 7.15⇥10 2N0.29+3.00⇥10 1D0.23+0.31K2=0.01 = 0.10DoptNopt  C=1021= 191.87Average 0.220.300.430.600.851.201.682.373.34 0.420.490.580.690.810.961.141.341.58 0.550.650.770.901.061.251.471.732.04 0.340.450.600.801.071.431.902.533.37 0.600.690.800.921.061.231.411.631.88 0.430.530.650.800.981.201.481.812.23 Figure 10: The ﬁt result of the scaling experiment with WSD Scheduler. The black dots in ahorizontal line denote the decayed checkpoints in different compute within the same modelsize.C-Eval CMMLU MMLU GSM8K MATH HumanEval MBPPA-1 40.0 41.5 44.6 27.7 5.1 27.7 24.4A-252.6 51.1 50.9 42.3 5.4 30.4 30.3B-1 40.9 41.5 47.9 34.2 7.9 43.9 30.5B-2 41.2 42.0 47.934.47.3 43.9 29.8B-349.1 46.8 49.631.810.5 44.5 32.8Table 1: The ablation study of different training strategies.6.1 Model DetailsVocabulary.We use two tokenizers of 122,753 vocabulary size for MiniCPM-2.4B and73,440 vocabulary for MiniCPM-1.2B. A small vocabulary for 1.2B favors efﬁciency withoutharming much performance. Details of the tokenizers are in AppendixC. Including theembedding parameters increases total parameters by 0.3B and 0.2B respectively.Shared Input-output Layer.For SLM, the embedding takes up a large parameter space. Tomake the model parameters smaller, we use the Embedding Sharing techniques for bothMiniCPM-2.4B and MiniCPM-1.2B.Deep-and-thin Network.We train MiniCPM-2.4B before training MiniCPM-1.2B. Whentraining MiniCPM-2.4B, we adopt a deeper and thinner architecture compared to Phi-ModelN (B)dmdffdhnqnkvLBatch size (M) Tokens (T)MiniCPM-1.2B1,247,442,432 1,536 3,840 64 24 8 52 2M⇐4M 1.1TMiniCPM-2.4B2,442,057,984 2,304 5,760 64 36 36 40 4M 1.1TTable 2: Model conﬁgurations for MiniCPM. N (B),dm,dff,dh,nq,nkv,L, Batch size (M),Tokens (T) represents the number of non-embedding parameters of the model, model hiddendimension, feedforward layer bottleneck dimension, attention head dimension, number ofqueries, number key/values, number of layers, training batch size, total training tokens.11

2(Javaheripi & Bubeck,2023) (40 layers compared to 32 layers). Recently,Liu et al.(2024)propose to train deep and thin networks for SLMs, which aligns with our perspective.Therefore, we further make the architecture deeper and thinner for MiniCPM-1.2B.Group Query Attention.We train MiniCPM-2.4B without modiﬁcation on the attentionlayer. Whereas we apply Group Query Attention (Ainslie et al.,2023) to MiniCPM-1.2B,inspired byLiu et al.(2024), to further reduce the parameters number.6.2 Training StagesThe overall training of the MiniCPM base model includes three stages: stable training stage,decay stage, SFT stage (Zhang et al.,2023;Wei et al.,2021). Throughout the stages, we useAdam Optimizer (Kingma & Ba,2014).Stable Training Stage.We utilize around 1T data (see Section11for data distribution),with the majority of the data sourced from open datasets. We use the optimal conﬁgurationdiscovered during the model wind tunnel experiments, WSD LRS, with a batch size of 3.93million and a max learning rate of 0.01.Decay Stage.We use a mixture of the pretraining data and high-quality SFT data. Forthe speciﬁc annealing form of the WSD scheduler, we employ exponential annealing, i.e.f(s↔T)=0.5(s↔S)/T, in whichTis set to be 5000 steps (20B tokens).SFT Stage.We ﬁnd it still necessary to conduct a separate SFT phase. We utilize SFT datasimilar to the annealing phase excluding pre-training data and train with approximately 6billion tokens. The learning rate for SFT is aligned with the one at the end of annealing, anda WSD Scheduler with exponential decay is also employed.6.3 Training Data Distribution Code Pretrain25.0CommonCrawl Chn25.0 Dolma24.0C415.0Pile8.0peS2o1.0Arxiv1.0Open Web Math1.0Data Mixture of Stable StageCode Pretrain19.6Dolma15.7CommonCrawl Chn14.6C49.5Wikipedia6.7Pile5.1SFT mixed4.8Baidu Baike4.5Code SFT3.8Book Chinese2.8Knowledge SFT2.5UltraChat2.0Math Synthetic1.9Other1.3Stack Exchange QA1.1Math SFTpeS2oArxivOpen Web MathLaw PretrainShareGPT4Logic SFTSimOrcaOssInstructEvolInstructData Mixture of Decay Stage Figure 11: Data mixture of different training stages. The stable stage is shown on the leftand the decay stage is shown on the right.We introduce our training data distribution in Figure11. In the ﬁgure, CommonCrawlChnin a Chinese Corpus is derived from CommonCrawl raw corpus and goes through thoroughcleaning. Dolma (Soldaini et al.,2024), C4 (Raffel et al.,2019), and Pile (Gao et al.,2020;Biderman et al.,2022) are English corpora. They are deduplicated inner corpus and acrosscorpus using MinHash algorithms (Broder,1997). The Code Pre-train data contains thestack (Kocetkov et al.,2022) and StarCoderLi et al.(2023a), with inner deduplication andcross deduplication. In the decay stage, the data mixture contains more diverse data andproprietary data, including UltraChat (Ding et al.,2023), SlimOrca (Lian et al.,2023a;b),OssInstruct (Wei et al.,2023), EvolInstruct (Xu et al.,2023). The data with the sufﬁx SFT isour proprietary data including LeetCode questions, Kindergarten through 12th grade (K12)textbooks and questions, etc.12

6.4 Training LossThe overall training loss on the C4 dataset is shown in Figure12. We can see that as expectedin the preliminary experiments, the loss decreases sharply in the decay stage. Since we usethe exponential decay, the loss still drops after the learning rate drops below 10% of the maxlearning rate. However, since we continue to SFT the model after the decay stage, we donot utilize the ﬁnal checkpoints. The checkpoints we ﬁnetune from are shown in the lastcheckpoint ofdark greensegment. The ﬁrst drop in MiniCPM-1.2B is the result of enlargingbatch size, which might have a similar effect as decreasing learning rate (Smith et al.,2017). 020040060080010001200Tokens (B)2.52.62.72.82.93.0Loss on C4020040060080010001200Tokens (B)2.42.52.62.72.82.93.0Loss on C4Figure 12: Loss curve on C4 dataset for MiniCPM-1.2B (Left) and MiniCPM-2.4B (Right).Theorangesegment at the tail of the loss curve represents the remaining decay process,which is not utilized in the released version of MiniCPM.6.5 EvaluationThe overall evaluation utilizes our open-source tool UltraEval3. UltraEval is an open-sourceframework for assessing the capabilities of foundation models. It provides a lightweight anduser-friendly evaluation system, supporting performance assessment for mainstream largemodels, and catering to the rapid evaluation needs of model training teams. The underlyinginference and acceleration use the open-source framework vLLM (Kwon et al.,2023), andthe dataset includes commonly used datasets: MMLU (Hendrycks et al.,2020) for Englishknowledge, CMMLU (Li et al.,2024) and C-Eval (Huang et al.,2024) for Chinese knowledge,HumanEval (Chen et al.,2021) and MBPP (Austin et al.,2021) for coding, GSM8K (Cobbeet al.,2021) and MATH (Hendrycks et al.,2021) for mathematics, and HellaSwag (Zellerset al.,2019), ARC-e (Clark et al.,2018), ARC-c (Clark et al.,2018) for commonsense reasoning,and BBH (Suzgun et al.,2022) for logic reasoning.Due to the difﬁculty of standardizing evaluations for large models and the lack of publiclyavailable prompts and test codes for many models’ evaluations, we try our best to adapt theevaluation methods to suit various model types. Speciﬁcally, we start from a standardizedinput prompt during testing and adjust it according to each model’s appropriate input-output template. Theevaluation scripts and prompts are also open-sourcein our repository,and we welcome developers to continually improve our evaluation methods.When testing QA tasks (ARC-e, ARC-c, HellaSwag), two approaches are typically employed.The ﬁrst involves using Perplexity (PPL): we extend each option as the continuation ofthe question and use the PPL of the option as the selection criterion. The second is directgeneration, where the model directly outputs answer options. We observe signiﬁcantdifferences in results obtained using these two methods. MiniCPM performs similarly indirect generation and PPL tests, with better performance in direct generation. On the otherhand, Mistral-7B-v0.1 performs better in PPL tests but exhibits poorer performance in directgeneration. To address this phenomenon, when reporting the scores for each model, weadopt the score from the evaluation method that yields the highest score, ensuring fairnessin comparison.3https://ultraeval.openbmb.cn/home13

Model C-EvalCMMLUMMLUHumanEvalMBPPGSM8KMATHLlama2-7B 32.42 31.11 44.32 12.20 27.17 13.57 1.80Qwen-7B 58.96 60.35 57.65 17.07 42.15 41.24 5.34Deepseek-7B 42.82 44.45 47.82 20.12 41.45 15.85 1.53Mistral-7B 46.12 42.96 62.69 27.44 45.20 33.13 5.00Gemma-7B 42.57 44.20 60.83 38.41 50.12 47.31 6.18Llama2-13B 37.32 37.06 54.71 17.07 32.55 21.15 2.25MPT-30B 29.34 32.09 46.56 21.95 35.36 10.31 1.56Falcon-40B 40.29 41.57 53.53 24.39 36.53 22.44 1.92TinyLlama-1.1B 25.02 24.03 24.3 6.71 19.91 2.27 0.74Qwen-1.8B 49.81 45.32 43.37 7.93 17.8 19.26 2.42Qwen1.5-1.8B55.0050.85 43.81 5.49 24.82 26.16 3.25Gemini Nano-3B - - - - 27.20 22.80 -StableLM-Zephyr-3B 30.34 30.89 45.90 35.37 31.85 52.54 12.12Phi-2(2B) 23.37 24.18 52.66 47.5655.04 57.163.50Gemma-2B 29.26 28.56 38.49 24.39 29.74 16.83 3.34MiniCPM-1.2B49.14 46.81 49.63 44.51 32.75 31.77 10.60MiniCPM-2.4B51.1351.07 53.46 50.0047.31 53.8310.24Model BBH ARC-e ARC-cHellaSwagAvg AvgenAvgchnLlama2-7B 33.2375.25†42.7575.62†35.40 36.21 31.77Qwen-7B 37.75 83.42 64.7675.32†49.46 47.19 59.66Deepseek-7B 33.3874.58†42.15†75.45†39.96 39.15 43.64Mistral-7B 41.06 83.92 70.7380.43†48.97 49.96 44.54Gemma-7B 39.19 89.35 76.79 79.47 52.22 54.18 43.39Llama2-13B 37.9278.87†58.1979.23†41.48 42.44 37.19MPT-30B 38.2278.66†46.08†79.72†38.17 39.82 30.72Falcon-40B 36.2481.94†57.6883.26†43.62 44.21 40.93TinyLlama-1.1B 28.7860.77†28.15†58.33†25.36 25.55 24.53Qwen-1.8B 29.0763.97†43.6959.28†34.72 31.87 47.57Qwen1.5-1.8B 28.82 64.86 45.56 59.39 37.09 33.5752.93Gemini Nano-3B 42.40 - - - - - -StableLM-Zephyr-3B 37.68 73.78 55.3871.87†43.46 46.32 30.62Phi-2(2B)43.39 86.11 71.2573.07†48.8454.4223.78Gemma-2B 30.93 74.33 40.70 69.51 35.10 36.47 28.91MiniCPM-1.2B34.70 80.93 66.8154.7245.67 45.16 47.98MiniCPM-2.4B36.87 85.44 68.00 68.2552.3352.60 51.10Table 3: Benchmark Score of MiniCPM-2.4B and MiniCPM-1.2B (both without RLHF). Thetwo tables are continuous horizontally.Avgis over all dataset in the table,Avgchnis theaverage of C-Eval and CMMLU whileAvgenis the average of remaining datasets.†meansthe result is tested using PPL metrics.Boldnumbers represent the best score among theSLMs. Results of Gemini Nano-3B are borrowed fromGemini et al.(2023). 14

The overall evaluation results are in Table4. Overall, on the mentioned datasets, we haveseveral observations. (1) On average, MiniCPM-2.4B ranks the highest among all the SLMs.(2) MiniCPM-2.4B performs similarly to Mistral-7B-v0.1 in English but signiﬁcantly out-performs Mistral-7B-v0.1 in Chinese. (3) MiniCPM-2.4B outperforms Llama2-13B exceptin MMLU, BBH, and HellaSwag, while MiniCPM-1.2B outperforms Llama2-7B exceptin HellaSwag. (4)Generally, BBH is harder for SLMs than LLMs compared to anotherknowledge-oriented dataset, demonstrating that reasoning ability might be more depen-dent on model size than knowledge. (5) Among SLMs, Phi-2 performance is on par withMiniCPM on academic-oriented datasets. This might be because their training data mostlyinvolves textbook-style data that emphasize educational and academic scenarios. Since ourpre-training data covers more distribution, we think MiniCPM is better at knowledge andability coverage, which can be seen in AppendixF.7 MiniCPM FamilyIn this section, we introduce the other models that build on MiniCPM base models. Specif-ically, we trained the aligned model, long-context model, and MoE model for MiniCPM2.4B.7.1 MiniCPM-DPOAfter SFT, we employ DPO (Rafailov et al.,2024) for human preference alignment of themodel. During this stage, UltraFeedback (Cui et al.,2023) is utilized as the primary align-ment dataset, and a proprietary preference dataset is constructed to enhance the model’scode and mathematical capabilities. We conduct one epoch of DPO training with a learningrate of 1↓10↔5and utilize a Cosine LRS since we have a pre-deﬁned training step.After applying DPO for preference alignment, the model’s score on MTBench (Zheng et al.,2024) increased from 6.89 after SFT to 7.25, surpassing even large models such as Llama2-70B-Chat (see Figure13). However, we also noticed that the performance on benchmarks isslightly compromised, which is known as the alignment tax (Askell et al.,2021).Model C-Eval CMMLU MMLU HumanEval MBPP GSM8K MATHChatGLM2-6B 52.05 49.21 45.77 10.37 9.38 22.74 5.96Mistral-7B-Instruct-v0.1 38.06 36.96 53.56 29.27 39.34 28.73 3.48Mistral-7B-Instruct-v0.2 42.55 41.92 60.51 36.59 48.95 40.49 4.95Qwen-7B-Chat 58.57 57.23 56.03 15.85 40.52 42.23 8.3Yi-6B-Chat 70.88 71.11 62.95 14.02 28.34 36.54 3.88Baichuan2-7B-Chat 53.28 53.50 53.00 21.34 32.32 25.25 6.32Deepseek-7B-chat 46.95 49.72 51.67 40.85 48.48 48.52 4.26Llama2-7B-Chat 34.54 32.64 47.64 14.02 27.40 21.15 2.08MiniCPM-2.4B-DPO48.64 48.37 53.05 51.22 48.01 53.37 9.86Model BBH ARC-e ARC-c HellaSwag Avg AvgenAvgchnChatGLM2-6B 32.60 74.45 56.82 58.48†37.98 35.17 50.63Mistral-7B-Instruct-v0.1 39.52 81.61 63.99 73.47†44.36 45.89 37.51Mistral-7B-Instruct-v0.2 39.81 86.28 73.38 84.55†50.91 52.83 42.24Qwen-7B-Chat 37.34 64.44†39.25†74.52†44.93 42.05 57.90Yi-6B-Chat 37.43 84.89 70.39 74.60†50.46 45.89 71.00Baichuan2-7B-Chat 37.46 79.63 60.15 69.23†44.68 42.74 53.39Deepseek-7B-chat 35.70 76.85 63.05 76.68†49.34 49.56 48.34Llama2-7B-Chat 35.54 74.28 54.78 75.65†38.16 39.17 33.59MiniCPM-2.4B-DPO36.22 85.02 68.17 65.67 51.60 52.29 48.51Table 4: Benchmark scores for MiniCPM-2.4B-DPO compared with larger chat models.15

Figure 13: MTBench score of MiniCPM-DPO-2.4B surpasses many models of larger size.7.2 MiniCPM-128KTasks involving lengthy contexts depend on the implicit information within these contexts,circumventing the need for the extensive knowledge often absent in SLMs. In this section,we expand the context length of MiniCPM-2.4B from 4,096 to 128,000 tokens, illustrating thecapability of SLM to effectively process long contexts.Initialization.For the initialization, we disable sharing embeddings between input andoutput, primarily to accommodate vocabulary parallelism essential for training with longcontext. The LM head is initialized from the input embedding.Training.Similar to MiniCPM, MiniCPM-2.4B-128K utilizes the WSD as its learning ratescheduler and reuses the last checkpoint of the stable training stage of MiniCPM-2.4B.Concerning training data, we categorize the dataset distribution detailed in Section6.3into “short data” and “long data”. We classify books, wikis, and papers as “long data”,and the other as the “short data”. The training comprises 44% long data and 56% shortdata for continued training. For the extension of long contexts, we apply Adjusted BaseFrequency (ABF) (Xiong et al.,2023) in the 4K to 32k range and employ NTK-Aware RoPEScaling (bloc97,2023) and curriculum learning from 32K to 128K. Both two stages involvefuture training. Furthermore, as indicated in Yi Tech Report (Young et al.,2024) andZebra (Song et al.,2023), we use of synthetic long QA data that signiﬁcantly enhances modelperformance in context-aware tasks.Evaluation.We evaluate MiniCPM-2.4B-128K in∞Bench (Zhang et al.,2024b), a pioneeringbenchmark for long context evaluations. The tasks in∞Bench (Zhang et al.,2024b) extendbeyond typical retrieval tasks and challenge the model with long context reasoning. We cansee in Table5, we achieve comparable results in Mistral-7B-Instruct-v0.2 (ABF1000w) andoutperform ChatGLM3-6B-128K despite being 2.5 times smaller.7.3 MiniCPM-MoEWe further extend the ability of MiniCPM using Mixture-of-Expert.Initialization.MiniCPM-MoE is initialized utilizing Sparse Upcycling (Komatsuzaki et al.,2022). The dense model checkpoint, derived from the stable phase of MiniCPM, undergoesa transformation wherein each MLP layer is substituted by an MoE layer. These new MoElayers are exact replicas of the original MLP layers from the dense checkpoint. The routerparameters are randomly initialized following a normal distribution with a mean of 0 and avariance of 0.01.Routing Mechanism.The number of total non-embedding parameters of MiniCPM-MoE is13.6B. During training and inference, two out of eight experts are activated for each token,16

[FIGURE PLACEHOLDER]
\includegraphics[width=\linewidth]{figure_13_placeholder}


ModelPasskeyNumberStringKV Re-trievalLongBookChoiceEngLongBookQAChnLongBookQAEngLongBookSumEngLWM-Text-128K 100 97.8 0.6 28.82 15.93 14.31 9.99Yarn-Mistral-7b-128K 92.71 56.61 0 27.95 15.49 9.55 9.06Mistral-7B-Instruct-v0.2(ABF 1000w)100 78.98 3.6 37.12 11.74 17.37 21.12Yi-6B-200K 100 94.92 0 36.68 15.07 9.2 0.92ChatGLM3-6B-128K 89.93 99.66 5.2 46.29 10.7 8.38 25.91MiniCPM-2.4B-128K98.31 99.83 9 29.69 23.06 16.33 15.73ModelLong Di-alogueQA EngMathCalcMathFindCodeDebugCodeRunAvgAvgw/oCode &MathLWM-Text-128k 1.5 0 3.43 20.05 1 24.45 33.62Yarn-Mistral-7b-128k 7.5 0 17.14 0.76 1.25 19.84 27.36Mistral-7B-Instruct-v0.2(ABF 1000w)9.5 0 29.43 17.51 0 27.75 36.9Yi-6B-200K 3.5 0 4.29 0.51 0.75 22.15 32.54ChatGLM3-6B-128K 6.5 0 8 5.33 1 25.58 36.57MiniCPM-2.4B-128K9.5 0 4.29 22.08 0 27.32 37.68Table 5: MiniCPM-2.4B-128K result in∞Bench (Zhang et al.,2024b)resulting in the number of activated parameters being approximately 4B. To prevent trainingfrom collapsing, an additional load balancing loss (Fedus et al.,2022) is applied to the ﬁnaltraining objective. This auxiliary loss is multiplied by 0.01 which is large enough to ensure abalanced distribution of tokens assigned to different experts.Training.Similar to MiniCPM, we employ WSD as our learning rate scheduler. Regardingthe training data, we adhere strictly to the distribution speciﬁed in Section6.3. The training batch size is maintained at 4M tokens during the stable training and decay stages and isreduced to 2M tokens during the SFT stage. The pre-training phase (including continue pre-train and decay stage) spans 130K steps, after which we notice diminishing improvement.The benchmark results are detailed in Table6.ModelC-Eval CMMLU MMLU HumanEval MBPP GSM8K MATH BBHLlama2-34B- - 62.6 22.6 33.0†42.2 6.2444.1Deepseek-MoE (16B)40.6 42.5 45.0 26.8 39.2 18.8 4.3 -Mistral-7B46.12 42.9662.6927.44 45.20 33.13 5.0 41.06Gemma-7B42.57 44.20 60.83 38.41 50.12 47.31 6.18 39.19MiniCPM-2.4B51.13 51.07 53.46 50.00 47.31 53.83 10.24 36.87MiniCPM-MoE (13.6B)58.11 58.8058.9056.71 51.05 61.56 10.5239.22Table 6: Benchmark results of MiniCPM-MoE.†means evaluation results on the full setof MBPP, instead of the hand-veriﬁed set (Austin et al.,2021). The evaluation results ofLlama2-34B and Qwen1.5-7B are taken from their technical reports.8 ConclusionThis paper introduces MiniCPM, comprising two SLMs with 2.4 B and 1.2 B non-embeddingparameters, respectively. These models demonstrate superior performance compared totheir larger counterparts. Our training methodologies are scalable both in terms of modeland data size, offering potential applicability in the development of LLMs. The introductionof our WSD scheduler is notable for promoting continuous training, exhibiting compellingtraining dynamics, and enabling efﬁcient study of scaling law. We further introduce theMiniCPM family, including DPO, long context, and MoE versions. Future directions includein-depth analysis of the loss decrease in the decay stage, and enhancing the capability ofMiniCPM by scaling in both model size and data size.17

Author Contributions
All authors contribute substantially to the MiniCPM project. Shengding Hu lead andparticipated in all aspects of the projects. This included the scaling experiments (conductedalongside Yuge Tu), babysitting the training of MiniCPM base models, and contributingto various other parts of the research. Shengding Hu wrote the paper. Chaoqun He wasresponsible for evaluating MiniCPM, while Ganqu Cui handled the RLHF training. XiangLong, Zhi Zheng, Xinrong Zhang and Shengding Hu extended the context window to128K. The MoE research was conducted by Yewei Fang and Zhi Zheng. Weilin Zhao andKaihuo Zhang contributed to the training and inference infrastructure. The open-sourcing ofMiniCPM was prepared by Yuxiang Huang and Shengding Hu. Shengding Hu, along withChenyang Zhao, also provided analysis on the WSD scheduler’s training dynamics. ZhengLeng Thai developed the tokenizer. The development of MiniCPM-V was carried out byChongyi Wang and Yuan Yao. The training corpus of MiniCPM was prepared by Jie Zhou, JieCai, Shengding Hu, Zhi Zheng, and Zhongwu Zhai. The paper was proofread by XingrongZhang and Chaoqun He. Insightful instructions on training MiniCPM were provided byXu Han, Ning Ding, and Zhiyuan Liu. Finally, Zhiyuan Liu, Maosong Sun, Guoyang Zeng,Chao Jia, and Dahai Li offered essential resources for the training of MiniCPM.LimitationsAlthough we have proposed a thorough study of the scaling law with SLMs, this paperdoes not extend to training an LLM to validate the scaling law. The application of WSDLRS on LLMs has not been fully explored to date. However, we remain optimistic about itspotential advantages.AcknowledgementMiniCPM was initially published as a technical blog on February 1st, 2024. Since then, wehave received numerous insightful feedback from the community, signiﬁcantly contributingto the development of this paper. We extend our gratitude to Chunting Zhou and ArmenAghajanyan for their valuable discussions. Special thanks go to Peiqin Sun and Yan Wang fortheir meticulous feedback on clarifying ambiguities in the blog. Additionally, we appreciatethe open-source community’s efforts in integrating MiniCPM into inference frameworkslike llama.cpp, etc. 18
